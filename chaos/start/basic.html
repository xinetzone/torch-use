
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>åŸºç¡€ &#8212; torch-book 0.0.1 æ–‡æ¡£</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/default.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "xinetzone/torch-book");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "ğŸ’¬ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../_static/translations.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="canonical" href="https://xinetzone.github.io/torch-book/chaos/start/basic.html" />
    <link rel="shortcut icon" href="../../_static/favicon.jpg"/>
    <link rel="index" title="ç´¢å¼•" href="../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../search.html" />
    <link rel="next" title="Pytorch æ•°å€¼å¥—ä»¶æ•™ç¨‹" href="ns.html" />
    <link rel="prev" title="å¿«é€Ÿä¸Šæ‰‹" href="index.html" /> 
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
     
<link
  rel="alternate"
  type="application/atom+xml"
  href="../../posts/atom.xml"
  title="Blog"
/>
 
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.jpg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   é¡¹ç›®ç®€ä»‹
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../tutorials/index.html">
   æ•™ç¨‹
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/basics/index.html">
     PyTorch åŸºç¡€
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/basics/quickstart.html">
       å¿«é€Ÿå…¥é—¨
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/basics/autogradqs.html">
       è‡ªåŠ¨å¾®åˆ†
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/object-detection/index.html">
     ç›®æ ‡æ£€æµ‹
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../tutorials/object-detection/yolo/index.html">
       YOLO ç³»åˆ—
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
      <label for="toctree-checkbox-4">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../tutorials/object-detection/yolo/intro.html">
         YOLO ç®€ä»‹
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../tutorials/object-detection/yolo/tutorials/index.html">
         YOLO æ•™ç¨‹
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../tutorials/notes/index.html">
     ç¬”è®°
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/notes/autograd-mechanics.html">
       è‡ªåŠ¨å¾®åˆ†æœºåˆ¶
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/notes/detection.html">
       ç›®æ ‡æ£€æµ‹
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/notes/extending.html">
       æ‰©å±• PyTorch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../tutorials/notes/thop.html">
       THOP: PyTorch OpCounter
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../tutorials/seg-fine-tuning.html">
     å¾®è°ƒåˆ†å‰²
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../quant/index.html">
   é‡åŒ–
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../quant/intro.html">
     é‡åŒ–ç®€ä»‹
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../quant/start.html">
     å¿«é€Ÿå…¥é—¨
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../openmmlab/index.html">
   MMDetection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../openmmlab/start.html">
     MMDect å¿«é€Ÿä¸Šæ‰‹
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../index.html">
   Eager é‡åŒ–(æ··ä¹±)
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="../intro.html">
     é‡åŒ–ç®€ä»‹ï¼ˆEagerï¼‰
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../recipes.html">
     é‡åŒ–èœè°±
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../quantized-tensor.html">
     é‡åŒ–å¼ é‡
    </a>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="index.html">
     å¿«é€Ÿä¸Šæ‰‹
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       åŸºç¡€
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="ns.html">
       Pytorch æ•°å€¼å¥—ä»¶æ•™ç¨‹
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../study/index.html">
     å­¦ä¹ 
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
    <label for="toctree-checkbox-10">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../study/intro.html">
       æ¦‚è¿°
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../study/transfer-learning/index.html">
       è¿ç§»å­¦ä¹ 
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
      <label for="toctree-checkbox-11">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/transfer-learning/basic.html">
         è®¡ç®—æœºè§†è§‰åˆ†ç±»
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/transfer-learning/quantized.html">
         é‡åŒ–è®¡ç®—æœºè§†è§‰åˆ†ç±»
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/transfer-learning/custom.html">
         è‡ªå®šä¹‰
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/transfer-learning/cifar.html">
         æµ‹è¯•
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/transfer-learning/tvm.html">
         TVM
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../study/advanced/index.html">
       é«˜çº§æ•™ç¨‹
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
      <label for="toctree-checkbox-12">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/advanced/custom.html">
         è‡ªå®šä¹‰é‡åŒ–
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/advanced/run.html">
         é€šç”¨é‡åŒ–æ¨¡å‹
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/advanced/qat.html">
         QAT
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../study/advanced/cifar.html">
         ç‰¹å®šäº cifar10 çš„é‡åŒ–ï¼ˆå¾…æ›´ï¼‰
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../study/qat-resnet18.html">
       QATï¼ˆresnet18ï¼‰
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../study/test.html">
       æµ‹è¯• QAT
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../study/draft.html">
       å›æ”¶ç«™
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../tutorial/index.html">
     æ•™ç¨‹
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="simple">
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../papers/index.html">
     è®ºæ–‡
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../papers/gholami2021survey.html">
       A Survey of Quantization Methods for Efficient Neural Network Inference
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ecosystem/index.html">
   ç”Ÿæ€ç³»ç»Ÿ
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ecosystem/intro.html">
     ç®€ä»‹
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../others/index.html">
   å…¶ä»–
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../others/colab.html">
     Colab è®­ç»ƒ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../others/en-zh.html">
     ä¸­è‹±äº’è¯‘
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../others/raw.html">
     æœªæ•´ç†çš„èµ„æ–™
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../refs.html">
   å‚è€ƒ
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../glossary/index.html">
   æœ¯è¯­è¡¨
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../glossary/numpy.html">
     NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../glossary/pytorch.html">
     PyTorch è¯æ±‡è¡¨
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <div>
ç‰ˆæƒæ‰€æœ‰Â Â©Â 2021 <a href="https://xinetzone.github.io/">xinetzone</a></div>
<div>ç”± <a href="https://ebp.jupyterbook.org/">EBP</a> æä¾›æŠ€æœ¯æ”¯æŒ</div>
<a href="https://torch-book.readthedocs.io/">ç‰ˆæœ¬åˆ‡æ¢</a>

            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/xinetzone/torch-book/main?urlpath=lab/tree/docs/chaos/start/basic.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/xinetzone/torch-book/blob/main/docs/chaos/start/basic.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/xinetzone/torch-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/xinetzone/torch-book/issues/new?title=Issue%20on%20page%20%2Fchaos/start/basic.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/xinetzone/torch-book/edit/main/docs/chaos/start/basic.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/chaos/start/basic.ipynb.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> å¯¼èˆª
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   æ˜ å°„å‡½æ•°
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   é‡åŒ–å‚æ•°
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   æ ¡å‡†
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   ä»¿å°„å’Œå¯¹ç§°é‡åŒ–æ–¹æ¡ˆ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   é€å¼ é‡å’Œé€é€šé“é‡åŒ–æ–¹æ¡ˆ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   åç«¯å¼•æ“
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#qconfig">
   <code class="docutils literal notranslate">
    <span class="pre">
     QConfig
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pytorch">
   åœ¨ PyTorch ä¸­
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#post-training-dynamic-weight-only-quantization">
     Post-Training Dynamic/Weight-only Quantization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#post-training-static-quantization-ptq">
     Post-Training Static Quantization (PTQ)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id12">
       æ€¥åˆ‡çš„æ¨¡å¼
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fx-graph">
       FX GRAPH æ¨¡å¼
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quantization-aware-training-qat">
     Quantization-aware Training (QAT)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
   æ•æ„Ÿæ€§åˆ†æ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id16">
   å¯¹æ‚¨å·¥ä½œæµç¨‹çš„å»ºè®®
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>åŸºç¡€</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> å¯¼èˆª </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   æ˜ å°„å‡½æ•°
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   é‡åŒ–å‚æ•°
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   æ ¡å‡†
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   ä»¿å°„å’Œå¯¹ç§°é‡åŒ–æ–¹æ¡ˆ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   é€å¼ é‡å’Œé€é€šé“é‡åŒ–æ–¹æ¡ˆ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   åç«¯å¼•æ“
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#qconfig">
   <code class="docutils literal notranslate">
    <span class="pre">
     QConfig
    </span>
   </code>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pytorch">
   åœ¨ PyTorch ä¸­
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#post-training-dynamic-weight-only-quantization">
     Post-Training Dynamic/Weight-only Quantization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#post-training-static-quantization-ptq">
     Post-Training Static Quantization (PTQ)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id12">
       æ€¥åˆ‡çš„æ¨¡å¼
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fx-graph">
       FX GRAPH æ¨¡å¼
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#quantization-aware-training-qat">
     Quantization-aware Training (QAT)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
   æ•æ„Ÿæ€§åˆ†æ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id16">
   å¯¹æ‚¨å·¥ä½œæµç¨‹çš„å»ºè®®
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                 <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>åŸºç¡€<a class="headerlink" href="#id1" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h1>
<p>å‚è€ƒï¼š<a class="reference external" href="https://pytorch.org/blog/quantization-in-practice/">Practical Quantization in PyTorch</a></p>
<p><span class="guilabel">NN é‡åŒ–ç›®æ ‡</span>ï¼šè¿è¡Œæ›´å¿«ã€å†…å­˜éœ€æ±‚æ›´ä½ã€‚</p>
<ul class="simple">
<li><p>é‡åŒ–æºäºä¿¡æ¯å‹ç¼©ï¼›åœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œå®ƒæŒ‡çš„æ˜¯é™ä½å…¶æƒé‡å’Œ/æˆ–æ¿€æ´»çš„æ•°å€¼ç²¾åº¦ã€‚</p></li>
<li><p>è¿‡åº¦å‚æ•°åŒ–çš„ DNN æœ‰æ›´å¤šçš„ <strong>è‡ªç”±åº¦</strong>ï¼Œè¿™ä½¿å®ƒä»¬æˆä¸ºä¿¡æ¯å‹ç¼©çš„è‰¯å¥½å€™é€‰å¯¹è±¡ <span id="id2">[<a class="reference internal" href="../../refs.html#id3" title="Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. 2021. arXiv:2103.13630.">Gholami, Kim, Dong, Yao, Mahoney, and Keutzer, 2021</a>]</span>ã€‚</p></li>
</ul>
<p>å½“é‡åŒ–æ¨¡å‹æ—¶ï¼Œé€šå¸¸ä¼šå‘ç”Ÿä¸¤ä»¶äº‹â€”â€”æ¨¡å‹å˜å¾—æ›´å°ï¼Œè¿è¡Œæ•ˆç‡æ›´é«˜ã€‚ç¡¬ä»¶ä¾›åº”å•†æ˜ç¡®åœ°å…è®¸æ›´å¿«åœ°å¤„ç† 8 ä½æ•°æ®ï¼ˆè€Œä¸æ˜¯ 32 ä½æ•°æ®ï¼‰ï¼Œä»è€Œè·å¾—æ›´é«˜çš„ <strong>ååé‡</strong> ï¼ˆthroughputï¼‰ã€‚æ›´å°çš„æ¨¡å‹å…·æœ‰æ›´ä½çš„å†…å­˜å ç”¨å’ŒåŠŸè€— <span id="id3">[<a class="reference internal" href="../../refs.html#id4" title="Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: a whitepaper. 2018. arXiv:1806.08342.">Krishnamoorthi, 2018</a>]</span>ï¼Œè¿™å¯¹äºè¾¹ç¼˜éƒ¨ç½²è‡³å…³é‡è¦ã€‚</p>
<section id="id4">
<h2>æ˜ å°„å‡½æ•°<a class="headerlink" href="#id4" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p>æ˜ å°„å‡½æ•°ï¼šå°†å€¼ä»æµ®ç‚¹æ•°æ˜ å°„åˆ°æ•´æ•°ç©ºé—´çš„å‡½æ•°ã€‚å¸¸ç”¨çš„æ˜ å°„å‡½æ•°æ˜¯ç”± <span class="math notranslate nohighlight">\(Q(r) = round(r/S + Z)\)</span> ç»™å‡ºçš„çº¿æ€§å˜æ¢ï¼Œå…¶ä¸­ä¸º <span class="math notranslate nohighlight">\(r\)</span> ä¸ºè¾“å…¥ï¼Œ<span class="math notranslate nohighlight">\(S, Z\)</span> ä¸ºé‡åŒ–å‚æ•°ï¼ˆquantization parametersï¼‰ã€‚ä¸ºäº†é‡æ–°è½¬æ¢ä¸ºæµ®ç‚¹ç©ºé—´ï¼Œåå‡½æ•°ç”± <span class="math notranslate nohighlight">\(\overline{r} = (Q(r) - Z) \cdot S\)</span> ç»™å‡ºï¼ˆè¢«ç§°ä¸º <strong>åé‡åŒ–</strong>ï¼Œå³ dequantizationï¼‰ã€‚</p>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p><span class="math notranslate nohighlight">\(\overline{r} \neq r\)</span>ï¼Œå®ƒä»¬ä¹‹é—´çš„å·®å¼‚æ„æˆäº†é‡åŒ–è¯¯å·®ã€‚</p>
</div>
</section>
<section id="id5">
<h2>é‡åŒ–å‚æ•°<a class="headerlink" href="#id5" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p>æ˜ å°„å‡½æ•°ç”±ç¼©æ”¾å› å­ <span class="math notranslate nohighlight">\(S\)</span> å’Œé›¶ç‚¹ <span class="math notranslate nohighlight">\(Z\)</span> æ‰€å‚æ•°åŒ–ã€‚<span class="math notranslate nohighlight">\(S\)</span> ä»…ä»…æ˜¯è¾“å…¥èŒƒå›´ä¸è¾“å‡ºèŒƒå›´çš„æ¯”å€¼ <span class="math notranslate nohighlight">\(S = \frac {\beta - \alpha}{\beta_q - \alpha_q}\)</span>ã€‚è¿™é‡Œ <span class="math notranslate nohighlight">\([\alpha, \beta]\)</span> æ˜¯è¾“å…¥çš„è£å‰ªï¼ˆclippingï¼‰èŒƒå›´ï¼Œå³å…è®¸è¾“å…¥çš„è¾¹ç•Œã€‚<span class="math notranslate nohighlight">\([\alpha_q, \beta_q]\)</span> æ˜¯å®ƒè¢«æ˜ å°„åˆ°çš„é‡åŒ–è¾“å‡ºç©ºé—´çš„èŒƒå›´ã€‚å¯¹äº 8 ä½é‡åŒ–ï¼Œè¾“å‡ºèŒƒå›´ <span class="math notranslate nohighlight">\(\beta_q - \alpha_q \leq 2^8 -1\)</span>ã€‚<span class="math notranslate nohighlight">\(Z = -(\frac {\alpha}{S} - \alpha_q)\)</span> ä½œä¸ºåç½®ï¼Œä»¥ç¡®ä¿è¾“å…¥ç©ºé—´ä¸­çš„ <span class="math notranslate nohighlight">\(0\)</span> å®Œå…¨æ˜ å°„åˆ°é‡åŒ–ç©ºé—´ä¸­çš„ <span class="math notranslate nohighlight">\(0\)</span>ã€‚</p>
</section>
<section id="id6">
<h2>æ ¡å‡†<a class="headerlink" href="#id6" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p>é€‰æ‹©è¾“å…¥è£å‰ªèŒƒå›´çš„è¿‡ç¨‹ç§°ä¸º <strong>æ ¡å‡†</strong> ï¼ˆcalibrationï¼‰ã€‚æœ€ç®€å•çš„æ–¹æ³•ï¼ˆä¹Ÿæ˜¯ PyTorch ä¸­çš„é»˜è®¤æ–¹æ³•ï¼‰æ˜¯è®°å½•æ­£åœ¨è¿è¡Œçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ï¼Œå¹¶å°†å®ƒä»¬èµ‹å€¼ç»™ <span class="math notranslate nohighlight">\(\alpha\)</span> å’Œ <span class="math notranslate nohighlight">\(\beta\)</span>ã€‚<a class="reference external" href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/calib.html">TensorRT</a> ä¹Ÿä½¿ç”¨ç†µæœ€å°åŒ–ï¼ˆKL æ•£åº¦ï¼‰ï¼Œå‡æ–¹è¯¯å·®æœ€å°åŒ–ï¼Œæˆ–è¾“å…¥èŒƒå›´çš„ç™¾åˆ†ä½æ•°ã€‚</p>
<p>åœ¨ PyTorch ä¸­ï¼Œ<a class="reference external" href="https://xinetzone.github.io/pytorch-book/api/pytorch/ao/quantization/generated/torch.ao.quantization.observer.ObserverBase.html#torch.ao.quantization.observer.ObserverBase" title="(åœ¨ Pytorch Book)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Observer</span></code></a> æ¨¡å—æ”¶é›†å…³äºè¾“å…¥å€¼çš„ç»Ÿè®¡ä¿¡æ¯å¹¶è®¡ç®— qparams <span class="math notranslate nohighlight">\(S,Z\)</span>ã€‚ä¸åŒçš„æ ¡å‡†æ–¹æ¡ˆä¼šäº§ç”Ÿä¸åŒçš„é‡åŒ–è¾“å‡ºï¼Œæœ€å¥½é€šè¿‡ç»éªŒéªŒè¯å“ªç§æ–¹æ¡ˆæœ€é€‚åˆæ‚¨çš„åº”ç”¨ç¨‹åºå’Œä½“ç³»ç»“æ„ã€‚</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.observer</span> <span class="kn">import</span> <span class="n">MinMaxObserver</span><span class="p">,</span> <span class="n">MovingAverageMinMaxObserver</span><span class="p">,</span> <span class="n">HistogramObserver</span>

<span class="c1"># è®¾ç½®è¾“å…¥</span>
<span class="n">C</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">normal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">normal</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">normal</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">C</span><span class="p">,</span> <span class="n">L</span><span class="p">)),</span>
          <span class="n">normal</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="n">C</span><span class="p">,</span> <span class="n">L</span><span class="p">))]</span>
<span class="n">inputs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[tensor([[-0.4145,  0.4905],
         [ 1.9405, -1.1051],
         [ 2.1657, -1.1413]]),
 tensor([[-1.2207,  0.1083],
         [-0.7169,  2.2443],
         [-0.3198, -0.2961]])]
</pre></div>
</div>
</div>
</div>
<p>è®¾ç½®è§‚æµ‹ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">observers</span> <span class="o">=</span> <span class="p">[</span><span class="n">MinMaxObserver</span><span class="p">(),</span>
             <span class="n">MovingAverageMinMaxObserver</span><span class="p">(),</span>
             <span class="n">HistogramObserver</span><span class="p">()]</span>
</pre></div>
</div>
</div>
</div>
<p>è®¡ç®—å¹¶æŸ¥çœ‹é‡åŒ–å‚æ•°ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">obs</span> <span class="ow">in</span> <span class="n">observers</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="n">obs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">obs</span><span class="o">.</span><span class="n">calculate_qparams</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MinMaxObserver (tensor([0.0136]), tensor([90], dtype=torch.int32))
MovingAverageMinMaxObserver (tensor([0.0130]), tensor([88], dtype=torch.int32))
HistogramObserver (tensor([0.0124]), tensor([72], dtype=torch.int32))
</pre></div>
</div>
</div>
</div>
</section>
<section id="id7">
<h2>ä»¿å°„å’Œå¯¹ç§°é‡åŒ–æ–¹æ¡ˆ<a class="headerlink" href="#id7" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p>ä»¿å°„ï¼ˆaffineï¼‰æˆ–éå¯¹ç§°é‡åŒ–ï¼ˆasymmetric quantizationï¼‰æ–¹æ¡ˆåˆ†é…è¾“å…¥èŒƒå›´çš„æœ€å°å’Œæœ€å¤§è§‚æµ‹å€¼ã€‚ä»¿å°„æ–¹æ¡ˆé€šå¸¸æä¾›æ›´å°çš„å‰ªåˆ‡èŒƒå›´ï¼Œå¹¶ä¸”å¯¹äºé‡åŒ–éè´Ÿæ¿€æ´»éå¸¸æœ‰ç”¨ï¼ˆå¦‚æœä½ çš„è¾“å…¥å¼ é‡æ°¸è¿œéƒ½ä¸æ˜¯è´Ÿçš„ï¼Œä½ å°±ä¸éœ€è¦è¾“å…¥èŒƒå›´åŒ…å«è´Ÿå€¼ï¼‰ã€‚è®¡ç®—èŒƒå›´ä¸º <span class="math notranslate nohighlight">\(\alpha=\min(r), \beta = \max(r)\)</span>ã€‚å½“ç”¨äºæƒå€¼å¼ é‡ <span id="id8">[<a class="reference internal" href="../../refs.html#id5" title="Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: principles and empirical evaluation. 2020. arXiv:2004.09602.">Wu, Judd, Zhang, Isaev, and Micikevicius, 2020</a>]</span> æ—¶ï¼Œä»¿å°„é‡åŒ–ä¼šå¯¼è‡´æ›´æ˜‚è´µçš„è®¡ç®—æ¨ç†ã€‚</p>
<p>å¯¹ç§°é‡åŒ–ï¼ˆSymmetric quantizationï¼‰æ–¹æ¡ˆå°†è¾“å…¥èŒƒå›´é›†ä¸­åœ¨ <span class="math notranslate nohighlight">\(0\)</span> é™„è¿‘ï¼Œæ¶ˆé™¤äº†è®¡ç®—é›¶ç‚¹åç½®çš„éœ€è¦ã€‚è®¡ç®—èŒƒå›´ä¸º <span class="math notranslate nohighlight">\(-\alpha=\beta=\max(|\max(r)|,|\min(r)|)\)</span>ã€‚</p>
<p>å¯¹äºå€¾æ–œçš„ä¿¡å·ï¼ˆå¦‚éè´Ÿæ¿€æ´»ï¼‰ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ç³Ÿç³•çš„é‡åŒ–åˆ†è¾¨ç‡ï¼ˆquantization resolutionï¼‰ï¼Œå› ä¸ºå‰ªè¾‘èŒƒå›´åŒ…æ‹¬ä»æœªåœ¨è¾“å…¥ä¸­å‡ºç°çš„å€¼ï¼ˆå‚è§ä¸‹é¢çš„ pyplotï¼‰ã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">def</span> <span class="nf">get_symmetric_range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;è·å–å¯¹ç§°èŒƒå›´&#39;&#39;&#39;</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">abs</span><span class="p">())</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">beta</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">beta</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">get_affine_range</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;è·å–ä»¿å°„èŒƒå›´&#39;&#39;&#39;</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">plt</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">scheme</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;ç”»å‡ºä¸åŒæ–¹æ¡ˆçš„åˆ†å¸ƒ&#39;&#39;&#39;</span>
    <span class="n">boundaries</span> <span class="o">=</span> <span class="n">get_affine_range</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="k">if</span> <span class="n">scheme</span> <span class="o">==</span> <span class="s1">&#39;affine&#39;</span> \
        <span class="k">else</span> <span class="n">get_symmetric_range</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">a</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">boundaries</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">ymax</span><span class="p">)</span>


<span class="c1"># æ¨¡æ‹Ÿæ¿€æ´»å’Œæƒé‡</span>
<span class="n">act</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">Pareto</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">))</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">normal</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">act</span><span class="p">,</span> <span class="s1">&#39;affine&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Activation, Affine-Quantized&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">act</span><span class="p">,</span> <span class="s1">&#39;symmetric&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Activation, Symmetric-Quantized&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">weights</span><span class="p">,</span> <span class="s1">&#39;affine&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Weights, Affine-Quantized&quot;</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">weights</span><span class="p">,</span> <span class="s1">&#39;symmetric&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Weights, Symmetric-Quantized&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>åœ¨ PyTorch ä¸­ï¼Œä½ å¯ä»¥åœ¨åˆå§‹åŒ– <code class="docutils literal notranslate"><span class="pre">Observer</span></code> æ—¶æŒ‡å®šä»¿å°„æˆ–å¯¹ç§°æ¨¡å¼ã€‚æ³¨æ„ï¼Œå¹¶éæ‰€æœ‰ <code class="docutils literal notranslate"><span class="pre">observer</span></code> éƒ½æ”¯æŒè¿™ä¸¤ç§æ–¹æ¡ˆã€‚</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">qscheme</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span><span class="p">]:</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">MovingAverageMinMaxObserver</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">qscheme</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
        <span class="n">obs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Qscheme: </span><span class="si">{</span><span class="n">qscheme</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">obs</span><span class="o">.</span><span class="n">calculate_qparams</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Qscheme: torch.per_tensor_affine | (tensor([0.0130]), tensor([88], dtype=torch.int32))
Qscheme: torch.per_tensor_symmetric | (tensor([0.0170]), tensor([128]))
</pre></div>
</div>
</div>
</div>
</section>
<section id="id9">
<h2>é€å¼ é‡å’Œé€é€šé“é‡åŒ–æ–¹æ¡ˆ<a class="headerlink" href="#id9" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p>é‡åŒ–å‚æ•°å¯ä»¥ä½œä¸ºæ•´ä½“è®¡ç®—å±‚çš„æ•´ä¸ªæƒå€¼å¼ é‡ï¼Œä¹Ÿå¯ä»¥å•ç‹¬è®¡ç®—æ¯ä¸ªé€šé“çš„æƒå€¼å¼ é‡ã€‚åœ¨æ¯å¼ é‡ä¸­ï¼Œå¯¹å±‚ä¸­çš„æ‰€æœ‰é€šé“åº”ç”¨ç›¸åŒçš„å‰ªåˆ‡èŒƒå›´ï¼š</p>
<p><img alt="" src="../../_images/tensor-quantization.png" /></p>
<p>å¯¹äºæƒå€¼é‡åŒ–ï¼Œé€é€šé“ï¼ˆPer-Channelï¼‰å¯¹ç§°é‡åŒ–æä¾›äº†æ›´å¥½çš„ç²¾åº¦ï¼›é€å¼ é‡ï¼ˆPer-Tensorï¼‰é‡åŒ–çš„æ€§èƒ½å¾ˆå·®ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºä¸åŒé€šé“ä¹‹é—´çš„è½¬æ¢æƒå€¼ä¸æ‰¹é‡èŒƒæ•°æŠ˜å ï¼ˆbatchnorm foldingï¼‰ <span id="id10">[<a class="reference internal" href="../../refs.html#id5" title="Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: principles and empirical evaluation. 2020. arXiv:2004.09602.">Wu, Judd, Zhang, Isaev, and Micikevicius, 2020</a>]</span> å·®å¼‚å¾ˆå¤§ã€‚</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.ao.quantization.observer</span> <span class="kn">import</span> <span class="n">MovingAveragePerChannelMinMaxObserver</span>
<span class="c1"># è®¡ç®—å…¨éƒ¨ `C` é€šé“çš„ qparams</span>
<span class="n">obs</span> <span class="o">=</span> <span class="n">MovingAveragePerChannelMinMaxObserver</span><span class="p">(</span><span class="n">ch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="n">obs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">calculate_qparams</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.0036, 0.0119, 0.0128]), tensor([119,  92,  88], dtype=torch.int32))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">obs</span> <span class="o">=</span> <span class="n">MovingAveragePerChannelMinMaxObserver</span><span class="p">(</span><span class="n">ch_axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="n">obs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">calculate_qparams</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor([0.0101, 0.0064]), tensor([ 42, 176], dtype=torch.int32))
</pre></div>
</div>
</div>
</div>
</section>
<section id="id11">
<h2>åç«¯å¼•æ“<a class="headerlink" href="#id11" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p>ç›®å‰ï¼Œé‡åŒ–ç®—å­é€šè¿‡ <a class="reference external" href="https://github.com/pytorch/FBGEMM">FBGEMM åç«¯</a> åœ¨ x86 æœºå™¨ä¸Šè¿è¡Œï¼Œæˆ–è€…åœ¨ ARM æœºå™¨ä¸Šä½¿ç”¨ <a class="reference external" href="https://github.com/pytorch/QNNPACK">QNNPACK</a> åŸè¯­ã€‚æœåŠ¡å™¨ GPU çš„åç«¯æ”¯æŒï¼ˆé€šè¿‡ TensorRT å’Œ cuDNNï¼‰å³å°†æ¨å‡ºã€‚äº†è§£æ›´å¤šå…³äºå°†é‡åŒ–æ‰©å±•åˆ°è‡ªå®šä¹‰åç«¯ï¼š<a class="reference external" href="https://github.com/pytorch/rfcs/blob/master/RFC-0019-Extending-PyTorch-Quantization-to-Custom-Backends.md">RFC-0019</a>ã€‚</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.ao.quantization.qconfig</span> <span class="kn">import</span> <span class="n">get_default_qconfig</span>
<span class="n">backend</span> <span class="o">=</span> <span class="s1">&#39;fbgemm&#39;</span>  <span class="c1"># if x86 else &#39;qnnpack&#39;</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">engine</span> <span class="o">=</span> <span class="n">backend</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="qconfig">
<h2><code class="docutils literal notranslate"><span class="pre">QConfig</span></code><a class="headerlink" href="#qconfig" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p><a class="reference external" href="https://xinetzone.github.io/pytorch-book/api/pytorch/ao/quantization/generated/torch.ao.quantization.qconfig.QConfig.html#torch.ao.quantization.qconfig.QConfig" title="(åœ¨ Pytorch Book)"><code class="xref py py-class docutils literal notranslate"><span class="pre">QConfig</span></code></a> NamedTuple å­˜å‚¨ç”¨äºé‡åŒ–æ¿€æ´»å’Œæƒé‡çš„ Observer å’Œé‡åŒ–æ–¹æ¡ˆã€‚</p>
<p>ä¸€å®šè¦ä¼ é€’ <code class="docutils literal notranslate"><span class="pre">Observer</span></code> ç±»ï¼ˆè€Œä¸æ˜¯å®ä¾‹ï¼‰ï¼Œæˆ–è€…å¯ä»¥è¿”å› <code class="docutils literal notranslate"><span class="pre">Observer</span></code> å®ä¾‹çš„å¯è°ƒç”¨å¯¹è±¡ã€‚ä½¿ç”¨ <code class="xref py py-func docutils literal notranslate"><span class="pre">with_args()</span></code> è¦†ç›–é»˜è®¤å‚æ•°ã€‚</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.ao.quantization.qconfig</span> <span class="kn">import</span> <span class="n">QConfig</span>

<span class="n">my_qconfig</span> <span class="o">=</span> <span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">MovingAverageMinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
        <span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">MovingAveragePerChannelMinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">my_qconfig</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>QConfig(activation=functools.partial(&lt;class &#39;torch.ao.quantization.observer.MovingAverageMinMaxObserver&#39;&gt;, qscheme=torch.per_tensor_affine){}, weight=functools.partial(&lt;class &#39;torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver&#39;&gt;, qscheme=torch.qint8){})
</pre></div>
</div>
</div>
</div>
</section>
<section id="pytorch">
<h2>åœ¨ PyTorch ä¸­<a class="headerlink" href="#pytorch" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p>PyTorch å…è®¸æ‚¨ä½¿ç”¨å‡ ç§ä¸åŒçš„æ–¹å¼æ¥é‡åŒ–æ‚¨çš„æ¨¡å‹ï¼š</p>
<ul class="simple">
<li><p>Eager æ¨¡å¼ v/s FX Graph æ¨¡å¼ï¼šå¦‚æœä½ æ›´å–œæ¬¢çµæ´»ä½†æ‰‹åŠ¨çš„ï¼Œæˆ–å—é™çš„è‡ªåŠ¨è¿‡ç¨‹</p></li>
<li><p>é™æ€ v/s åŠ¨æ€ï¼šå¦‚æœé‡åŒ–æ¿€æ´»ï¼ˆå±‚çš„è¾“å‡ºï¼‰çš„ <code class="docutils literal notranslate"><span class="pre">qparams</span></code> ä¸ºæ‰€æœ‰è¾“å…¥é¢„å…ˆè®¡ç®—ï¼Œæˆ–å¯¹æ¯ä¸ªè¾“å…¥é‡æ–°è®¡ç®—ï¼Œ</p></li>
<li><p>é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆquantization-aware trainingï¼‰ v/s è®­ç»ƒåé‡åŒ–ï¼ˆpost-training quantizationï¼‰ï¼šå¦‚æœ <code class="docutils literal notranslate"><span class="pre">qparams</span></code> æ˜¯åœ¨æœ‰æˆ–æ²¡æœ‰é‡æ–°è®­ç»ƒçš„æƒ…å†µä¸‹è®¡ç®—çš„</p></li>
</ul>
<p>FX Graph Mode è‡ªåŠ¨èåˆç¬¦åˆæ¡ä»¶çš„æ¨¡å—ï¼Œæ’å…¥ Quant/DeQuant stubï¼Œæ ¡å‡†æ¨¡å‹å¹¶è¿”å›é‡åŒ–æ¨¡å—â€”â€”æ‰€æœ‰è¿™äº›éƒ½æ˜¯åœ¨ä¸¤ä¸ªæ–¹æ³•è°ƒç”¨ä¸­è¿›è¡Œçš„â€”â€”ä½†ä»…é€‚ç”¨äº <a class="reference external" href="https://pytorch.org/docs/stable/fx.html#torch.fx.symbolic_trace">å¯ç¬¦å·è·Ÿè¸ª</a> çš„ç½‘ç»œã€‚</p>
<p>åœ¨ DNN ä¸­ï¼Œé‡åŒ–çš„åˆé€‚å€™é€‰å¯¹è±¡æ˜¯ FP32 æƒå€¼ï¼ˆå±‚å‚æ•°ï¼‰å’Œæ¿€æ´»ï¼ˆå±‚è¾“å‡ºï¼‰ã€‚é‡åŒ–æƒå€¼å¯ä»¥å‡å°‘æ¨¡å‹çš„å¤§å°ã€‚é‡åŒ–æ¿€æ´»é€šå¸¸ä¼šå¯¼è‡´æ›´å¿«çš„æ¨ç†ã€‚</p>
<p>ä¾‹å¦‚ï¼Œ50 å±‚ ResNet ç½‘ç»œæœ‰è¿‘ 2600 ä¸‡ä¸ªæƒå€¼å‚æ•°ï¼Œåœ¨æ­£å‘ä¼ ç¨‹ä¸­è®¡ç®—è¿‘ 1600 ä¸‡ä¸ªæ¿€æ´»ã€‚</p>
<section id="post-training-dynamic-weight-only-quantization">
<h3>Post-Training Dynamic/Weight-only Quantization<a class="headerlink" href="#post-training-dynamic-weight-only-quantization" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h3>
<p>è¿™é‡Œæ¨¡å‹çš„æƒå€¼æ˜¯é¢„é‡åŒ–çš„ï¼›åœ¨æ¨ç†æœŸé—´ï¼Œæ¿€æ´»æ˜¯åŠ¨æ€é‡åŒ–çš„ã€‚è¿™æ˜¯æ‰€æœ‰æ–¹æ³•ä¸­æœ€ç®€å•çš„ä¸€ç§ï¼Œå®ƒåœ¨ <a class="reference external" href="https://xinetzone.github.io/pytorch-book/api/pytorch/ao/quantization/generated/torch.ao.quantization.quantize.quantize_dynamic.html#torch.ao.quantization.quantize.quantize_dynamic" title="(åœ¨ Pytorch Book)"><code class="xref py py-func docutils literal notranslate"><span class="pre">quantize_dynamic()</span></code></a> ä¸­æœ‰ä¸€è¡Œ API è°ƒç”¨ã€‚ç›®å‰åªæ”¯æŒçº¿æ€§å’Œå¾ªç¯ï¼ˆLSTMã€GRUã€RNNï¼‰å±‚è¿›è¡ŒåŠ¨æ€é‡åŒ–ã€‚</p>
<ul class="simple">
<li><p>å¯ä»¥å¯¼è‡´æ›´é«˜çš„ç²¾åº¦ï¼Œå› ä¸ºæ¯ä¸ªè¾“å…¥çš„è£å‰ªèŒƒå›´æ˜¯ç²¾ç¡®æ ¡å‡†çš„</p></li>
<li><p>å¯¹äºåƒ LSTM å’Œ Transformer è¿™æ ·çš„æ¨¡å‹ï¼ŒåŠ¨æ€é‡åŒ–æ˜¯é¦–é€‰çš„ï¼Œå› ä¸ºä»å†…å­˜ä¸­å†™å…¥/æ£€ç´¢æ¨¡å‹çš„æƒå€¼ä¼šå—åˆ¶äºå¸¦å®½</p></li>
<li><p>åœ¨è¿è¡Œæ—¶å¯¹æ¯ä¸ªå±‚çš„æ¿€æ´»è¿›è¡Œæ ¡å‡†å’Œé‡åŒ–ä¼šå¢åŠ è®¡ç®—å¼€é”€ã€‚</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># å° model</span>
<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">8</span><span class="p">,)),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">m</span>
</pre></div>
</div>
</div>
</div>
<p class="rubric">eager æ¨¡å¼</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.ao.quantization.quantize</span> <span class="kn">import</span> <span class="n">quantize_dynamic</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_dynamic</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">m</span><span class="p">,</span> <span class="n">qconfig_spec</span><span class="o">=</span><span class="p">{</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">model_quantized</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Conv2d(2, 64, kernel_size=(8,), stride=(1, 1))
  (1): ReLU()
  (2): DynamicQuantizedLinear(in_features=16, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)
  (3): DynamicQuantizedLSTM(10, 10)
)
</pre></div>
</div>
</div>
</div>
<p class="rubric">FX æ¨¡å¼</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.ao.quantization</span> <span class="kn">import</span> <span class="n">quantize_fx</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.qconfig</span> <span class="kn">import</span> <span class="n">default_dynamic_qconfig</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># ç©ºé”®è¡¨ç¤ºåº”ç”¨äºæ‰€æœ‰æ¨¡å—çš„é»˜è®¤å€¼</span>
<span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">default_dynamic_qconfig</span><span class="p">}</span>
<span class="n">model_prepared</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">prepare_fx</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/pc/xinet/anaconda3/envs/torchx/lib/python3.10/site-packages/torch/ao/quantization/fx/quantization_patterns.py:630: UserWarning: dtype combination: (torch.float32, torch.qint8, torch.quint8) is not supported by Conv supported dtype combinations are: [(torch.quint8, torch.qint8, None)]
  warnings.warn(
</pre></div>
</div>
</div>
</div>
</section>
<section id="post-training-static-quantization-ptq">
<h3>Post-Training Static Quantization (PTQ)<a class="headerlink" href="#post-training-static-quantization-ptq" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h3>
<p>PTQ ä¹Ÿé¢„é‡åŒ–æ¨¡å‹æƒé‡ï¼Œä½†ä¸æ˜¯åŠ¨æ€æ ¡å‡†æ¿€æ´»ï¼Œè€Œæ˜¯ä½¿ç”¨éªŒè¯æ•°æ®å¯¹å‰ªåˆ‡èŒƒå›´è¿›è¡Œé¢„æ ¡å‡†å’Œå›ºå®šï¼ˆâ€œé™æ€â€ï¼‰ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¿€æ´»åœ¨è¿ç®—ä¹‹é—´ä¿æŒé‡åŒ–ç²¾åº¦ã€‚å¤§çº¦ 100 ä¸ªå°æ‰¹æ¬¡çš„ä»£è¡¨æ€§æ•°æ®å°±è¶³ä»¥æ ¡å‡†è§‚æµ‹è€…ã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œä¸‹é¢çš„ä¾‹å­åœ¨æ ¡å‡†ä¸­ä½¿ç”¨äº†éšæœºæ•°æ®â€”â€”åœ¨åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨éšæœºæ•°æ®å°†å¯¼è‡´é”™è¯¯çš„ <code class="docutils literal notranslate"><span class="pre">qparams</span></code>ã€‚</p>
<p><img alt="" src="../../_images/ptq-flowchart.svg" /></p>
<p><a class="reference external" href="https://pytorch.org/tutorials/recipes/fuse.html">æ¨¡å—èåˆ</a> å°†å¤šä¸ªé¡ºåºæ¨¡å—ï¼ˆå¦‚ï¼š<code class="docutils literal notranslate"><span class="pre">[Conv2d,</span> <span class="pre">BatchNorm,</span> <span class="pre">ReLU]</span></code>ï¼‰ç»„åˆæˆä¸€ä¸ªã€‚èåˆæ¨¡å—æ„å‘³ç€ç¼–è¯‘å™¨åªéœ€è¦è¿è¡Œä¸€ä¸ªå†…æ ¸è€Œä¸æ˜¯å¤šä¸ªï¼›è¿™å¯ä»¥é€šè¿‡å‡å°‘é‡åŒ–è¯¯å·®æ¥æé«˜é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚</p>
<ul class="simple">
<li><p>é™æ€é‡åŒ–æ¯”åŠ¨æ€é‡åŒ–å…·æœ‰æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå› ä¸ºå®ƒæ¶ˆé™¤äº†å±‚ä¹‹é—´çš„ float&lt;-&gt;int è½¬æ¢æˆæœ¬ã€‚</p></li>
<li><p>é™æ€é‡åŒ–æ¨¡å‹å¯èƒ½éœ€è¦å®šæœŸé‡æ–°æ ¡å‡†ï¼Œä»¥ä¿æŒå¯¹åˆ†å¸ƒæ¼‚ç§»çš„é²æ£’æ€§ã€‚</p></li>
</ul>
<p>é™æ€é‡åŒ–æ¨¡å‹åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š</p>
<ul class="simple">
<li><p>èåˆæ¨¡å—</p></li>
<li><p>æ’å…¥ Quant/DeQuant å­˜æ ¹</p></li>
<li><p>å‡†å¤‡èåˆæ¨¡å—ï¼ˆåœ¨å±‚å‰å’Œå±‚åæ’å…¥è§‚å¯Ÿè€…ï¼‰</p></li>
<li><p>æ ¡å‡†å‡†å¤‡å¥½çš„æ¨¡å—ï¼ˆä¼ é€’ä»£è¡¨æ•°æ®ï¼‰</p></li>
<li><p>è½¬æ¢æ ¡å‡†æ¨¡å—ï¼ˆæ›¿æ¢ä¸ºé‡åŒ–ç‰ˆæœ¬ï¼‰</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.quantization</span> <span class="kn">import</span> <span class="n">quantize_fx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># å¦‚æœåœ¨ARMä¸Šè¿è¡Œï¼Œä½¿ç”¨ `qnnpack`ã€‚</span>
<span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;fbgemm&quot;</span>  <span class="c1"># è¿è¡Œåœ¨ x86 CPU ä¸Šã€‚</span>


<span class="k">def</span> <span class="nf">create_model</span><span class="p">():</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>
</pre></div>
</div>
</div>
</div>
<section id="id12">
<h4>æ€¥åˆ‡çš„æ¨¡å¼<a class="headerlink" href="#id12" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h4>
<p><strong>èåˆ</strong>ï¼šå°±åœ°èåˆç”¨æ‰€è¿°èåˆæ¨¡å—æ›¿æ¢æ‰€è¿°åºåˆ—ä¸­çš„ç¬¬ä¸€ä¸ªæ¨¡å—ï¼Œå…¶ä½™ç”¨ç›¸åŒæ¨¡å—æ›¿æ¢ã€‚</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="c1"># fuse first Conv-ReLU pair</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># fuse second Conv-ReLU pair</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s1">&#39;3&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): ConvReLU2d(
    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
  )
  (1): Identity()
  (2): ConvReLU2d(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
  )
  (3): Identity()
)
</pre></div>
</div>
</div>
</div>
<p>æ’å…¥ stubï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantStub</span><span class="p">(),</span>
                  <span class="o">*</span><span class="n">m</span><span class="p">,</span>
                  <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">DeQuantStub</span><span class="p">())</span>
<span class="n">m</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): QuantStub()
  (1): ConvReLU2d(
    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
  )
  (2): Identity()
  (3): ConvReLU2d(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
  )
  (4): Identity()
  (5): DeQuantStub()
)
</pre></div>
</div>
</div>
</div>
<p>å‡†å¤‡ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.ao.quantization.qconfig</span> <span class="kn">import</span> <span class="n">get_default_qconfig</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization.quantize</span> <span class="kn">import</span> <span class="n">prepare</span>

<span class="n">m</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
<span class="n">prepare</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/pc/xinet/anaconda3/envs/torchx/lib/python3.10/site-packages/torch/ao/quantization/observer.py:177: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): QuantStub(
    (activation_post_process): HistogramObserver()
  )
  (1): ConvReLU2d(
    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
    (activation_post_process): HistogramObserver()
  )
  (2): Identity()
  (3): ConvReLU2d(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
    (activation_post_process): HistogramObserver()
  )
  (4): Identity()
  (5): DeQuantStub()
)
</pre></div>
</div>
</div>
</div>
<p><strong>æ ¡å‡†</strong>ï¼šä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œè¿™ä¸ªä¾‹å­ä½¿ç”¨äº†éšæœºæ•°æ®ã€‚ä½¿ç”¨ä»£è¡¨æ€§ï¼ˆéªŒè¯ï¼‰æ•°æ®ä»£æ›¿ã€‚</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># with torch.inference_mode(): # PyTorch 1.9</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
        <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>è½¬æ¢ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)
  (1): QuantizedConvReLU2d(2, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0100000761449337, zero_point=0)
  (2): Identity()
  (3): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004895982798188925, zero_point=0)
  (4): Identity()
  (5): DeQuantize()
)
</pre></div>
</div>
</div>
</div>
<p>æ£€æŸ¥ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1 å­—èŠ‚ï¼Œè€Œä¸æ˜¯ FP32 çš„ 4 å­—èŠ‚</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">()</span><span class="o">.</span><span class="n">element_size</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1
</pre></div>
</div>
</div>
</div>
</section>
<section id="fx-graph">
<h4>FX GRAPH æ¨¡å¼<a class="headerlink" href="#fx-graph" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.ao.quantization.qconfig</span> <span class="kn">import</span> <span class="n">get_default_qconfig</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization</span> <span class="kn">import</span> <span class="n">quantize_fx</span>


<span class="k">def</span> <span class="nf">calibrate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;ä½¿ç”¨ä»£è¡¨æ€§ï¼ˆéªŒè¯ï¼‰æ•°æ®æ¥æ ¡å‡†&#39;&#39;&#39;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="c1"># with torch.inference_mode():</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">image</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ptq</span><span class="p">(</span><span class="n">float_model</span><span class="p">,</span> <span class="n">sample_inference_data</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">):</span>
    <span class="n">qconfig</span> <span class="o">=</span> <span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
    <span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="n">qconfig</span><span class="p">}</span>
    <span class="n">float_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">prepared_model</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">prepare_fx</span><span class="p">(</span><span class="n">float_model</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
    <span class="c1"># è¿è¡Œæ ¡å‡†</span>
    <span class="n">calibrate</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">,</span> <span class="n">sample_inference_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prepared_model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.ao.quantization.qconfig</span> <span class="kn">import</span> <span class="n">get_default_qconfig</span>
<span class="kn">from</span> <span class="nn">torch.ao.quantization</span> <span class="kn">import</span> <span class="n">quantize_fx</span>

<span class="k">def</span> <span class="nf">data_iter</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">_</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
<span class="n">model_prepared</span> <span class="o">=</span> <span class="n">ptq</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">data_iter</span><span class="p">(),</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;fbgemm&#39;</span><span class="p">)</span>
<span class="c1"># é‡åŒ–</span>
<span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="quantization-aware-training-qat">
<h3>Quantization-aware Training (QAT)<a class="headerlink" href="#quantization-aware-training-qat" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h3>
<p><img alt="" src="../../_images/qat-flowchart.svg" /></p>
<p>PTQ æ–¹æ³•å¯¹äºå¤§å‹æ¨¡å‹éå¸¸å¥½ï¼Œä½†åœ¨è¾ƒå°çš„æ¨¡å‹ä¸­å‡†ç¡®æ€§ä¼šå—åˆ°å½±å“ã€‚å½“ç„¶ï¼Œè¿™æ˜¯ç”±äºå°† FP32 çš„æ¨¡å‹è°ƒæ•´åˆ° INT8 åŸŸæ—¶çš„æ•°å€¼ç²¾åº¦æŸå¤±ã€‚</p>
<p>QAT é€šè¿‡åœ¨è®­ç»ƒæŸå¤±ä¸­åŒ…å«é‡åŒ–è¯¯å·®æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå› æ­¤è®­ç»ƒä¸€ä¸ª INT8-first æ¨¡å‹ã€‚</p>
<p><img alt="" src="../../_images/ptq-qat.png" /></p>
<p>æ‰€æœ‰çš„æƒé‡å’Œåç½®éƒ½å­˜å‚¨åœ¨ FP32 ä¸­ï¼Œåå‘ä¼ æ’­ç…§å¸¸å‘ç”Ÿã€‚ç„¶è€Œåœ¨æ­£å‘ä¼ é€’ä¸­ï¼Œé‡åŒ–æ˜¯é€šè¿‡ <code class="docutils literal notranslate"><span class="pre">FakeQuantize</span></code> æ¨¡å—è¿›è¡Œå†…éƒ¨æ¨¡æ‹Ÿçš„ã€‚å®ƒä»¬ä¹‹æ‰€ä»¥è¢«ç§°ä¸ºå‡çš„ï¼Œæ˜¯å› ä¸ºå®ƒä»¬å¯¹æ•°æ®è¿›è¡Œé‡åŒ–å’Œç«‹å³åé‡åŒ–ï¼Œå¹¶æ·»åŠ ä¸é‡åŒ–æ¨ç†è¿‡ç¨‹ä¸­å¯èƒ½é‡åˆ°çš„ç±»ä¼¼çš„é‡åŒ–å™ªå£°ã€‚å› æ­¤ï¼Œæœ€ç»ˆçš„æŸå¤±å¯ä»¥è§£é‡Šä»»ä½•é¢„æœŸçš„é‡åŒ–è¯¯å·®ã€‚åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œå¯ä»¥ä½¿æ¨¡å‹åœ¨æŸå¤±å‡½æ•°ä¸­è¯†åˆ«å‡ºæ›´å®½çš„åŒºåŸŸï¼Œå¹¶è¯†åˆ«å‡º FP32 å‚æ•°ï¼Œè¿™æ ·é‡åŒ–åˆ° INT8 ä¸ä¼šæ˜¾è‘—å½±å“ç²¾åº¦ã€‚</p>
<p><a class="reference external" href="https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt"><img alt="" src="../../_images/qat-fake-quantization.png" /></a></p>
<ul class="simple">
<li><p>QAT æ¯” PTQ å…·æœ‰æ›´é«˜çš„ç²¾åº¦ã€‚</p></li>
<li><p>Qparams å¯ä»¥åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´å­¦ä¹ ï¼Œä»¥è·å¾—æ›´ç»†ç²’åº¦çš„å‡†ç¡®æ€§ï¼ˆå‚è§ <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/_learnable_fake_quantize.py">LearnableFakeQuantize</a>ï¼‰ã€‚</p></li>
<li><p>åœ¨ QAT ä¸­ï¼Œé‡æ–°è®­ç»ƒä¸€ä¸ªæ¨¡å‹çš„è®¡ç®—æˆæœ¬å¯ä»¥è¾¾åˆ°å‡ ç™¾ä¸ª epochã€‚<span id="id13">[<a class="reference internal" href="../../refs.html#id3" title="Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. 2021. arXiv:2103.13630.">Gholami, Kim, Dong, Yao, Mahoney, and Keutzer, 2021</a>]</span></p></li>
</ul>
<p>é™¤äº†åœ¨å°†æ¨¡å‹å®é™…è½¬æ¢ä¸ºé‡åŒ–ç‰ˆæœ¬ä¹‹å‰çš„è®­ç»ƒå¾ªç¯ä¹‹å¤–ï¼ŒQAT éµå¾ªä¸ PTQ ç›¸åŒçš„æ­¥éª¤ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># è¿è¡Œåœ¨ x86 CPU ä¸Šã€‚å¦‚æœåœ¨ ARM ä¸Šè¿è¡Œï¼Œä½¿ç”¨ &quot;qnnpack&quot;ã€‚</span>
<span class="n">backend</span> <span class="o">=</span> <span class="s2">&quot;fbgemm&quot;</span>

<span class="n">m</span> <span class="o">=</span> <span class="n">create_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>èåˆï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># èåˆç¬¬ä¸€å¯¹ Conv-ReLU</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s1">&#39;3&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># èåˆç¬¬äºŒå¯¹ Conv-ReLU</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): ConvReLU2d(
    (0): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
  )
  (1): Identity()
  (2): ConvReLU2d(
    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))
    (1): ReLU()
  )
  (3): Identity()
)
</pre></div>
</div>
</div>
</div>
<p>æ’å…¥å­˜æ ¹ï¼ˆæ‰“æ¡©ï¼‰ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantStub</span><span class="p">(),</span>
                  <span class="o">*</span><span class="n">m</span><span class="p">,</span>
                  <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">DeQuantStub</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>å‡†å¤‡ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">m</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare_qat</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): QuantStub(
    (activation_post_process): HistogramObserver()
  )
  (1): ConvReLU2d(
    2, 64, kernel_size=(3, 3), stride=(1, 1)
    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))
    (activation_post_process): HistogramObserver()
  )
  (2): Identity()
  (3): ConvReLU2d(
    64, 128, kernel_size=(3, 3), stride=(1, 1)
    (weight_fake_quant): PerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))
    (activation_post_process): HistogramObserver()
  )
  (4): Identity()
  (5): DeQuantStub()
)
</pre></div>
</div>
</div>
</div>
<p>å¾ªç¯è®­ç»ƒï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">tgt</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">tgt</span><span class="o">-</span><span class="n">out</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>


<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>è½¬æ¢ï¼š</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential(
  (0): Quantize(scale=tensor([0.0080]), zero_point=tensor([0]), dtype=torch.quint8)
  (1): QuantizedConvReLU2d(2, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.0106028001755476, zero_point=0)
  (2): Identity()
  (3): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.004767395555973053, zero_point=0)
  (4): Identity()
  (5): DeQuantize()
)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id14">
<h2>æ•æ„Ÿæ€§åˆ†æ<a class="headerlink" href="#id14" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p>å¹¶ä¸æ˜¯æ‰€æœ‰å±‚å¯¹é‡åŒ–çš„å“åº”éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæœ‰äº›å±‚å¯¹ç²¾åº¦ä¸‹é™æ¯”å…¶ä»–å±‚æ›´æ•æ„Ÿã€‚ç¡®å®šæœ€ä¼˜çš„å±‚ç»„åˆä»¥æœ€å°åŒ–ç²¾åº¦ä¸‹é™æ˜¯éå¸¸è€—æ—¶çš„ï¼Œå› æ­¤ <span id="id15">[<a class="reference internal" href="../../refs.html#id5" title="Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius. Integer quantization for deep learning inference: principles and empirical evaluation. 2020. arXiv:2004.09602.">Wu, Judd, Zhang, Isaev, and Micikevicius, 2020</a>]</span> å»ºè®®è¿›è¡Œä¸€æ¬¡ä¸€æ¬¡çš„çµæ•åº¦åˆ†æï¼Œä»¥ç¡®å®šå“ªäº›å±‚æœ€æ•æ„Ÿï¼Œå¹¶åœ¨è¿™äº›å±‚ä¸Šä¿æŒ FP32 çš„ç²¾åº¦ã€‚åœ¨ä»–ä»¬çš„å®éªŒä¸­ï¼Œè·³è¿‡ 2 ä¸ª conv å±‚ï¼ˆåœ¨ MobileNet v1 çš„ 28 ä¸ª conv å±‚ä¸­ï¼‰ä½¿ä»–ä»¬æ¥è¿‘ FP32 çš„ç²¾åº¦ã€‚ä½¿ç”¨ FX Graph æ¨¡å¼ï¼Œå¯ä»¥åˆ›å»ºè‡ªå®šä¹‰ <code class="docutils literal notranslate"><span class="pre">qconfigs</span></code> æ¥è½»æ¾åšåˆ°è¿™ä¸€ç‚¹ã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># ONE-AT-A-TIME SENSITIVITY ANALYSIS </span>

<span class="k">for</span> <span class="n">quantized_layer</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Only quantizing layer: &quot;</span><span class="p">,</span> <span class="n">quantized_layer</span><span class="p">)</span>

  <span class="c1"># The module_name key allows module-specific qconfigs. </span>
  <span class="n">qconfig_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> 
  <span class="s2">&quot;module_name&quot;</span><span class="p">:[(</span><span class="n">quantized_layer</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_default_qconfig</span><span class="p">(</span><span class="n">backend</span><span class="p">))]}</span>

  <span class="n">model_prepared</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">prepare_fx</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig_dict</span><span class="p">)</span>
  <span class="c1"># calibrate</span>
  <span class="n">model_quantized</span> <span class="o">=</span> <span class="n">quantize_fx</span><span class="o">.</span><span class="n">convert_fx</span><span class="p">(</span><span class="n">model_prepared</span><span class="p">)</span>
  <span class="c1"># evaluate(model)</span>
</pre></div>
</div>
<p>å¦ä¸€ç§æ–¹æ³•æ˜¯æ¯”è¾ƒ FP32 å’Œ INT8 å±‚çš„ç»Ÿè®¡æ•°æ®ï¼›å¸¸ç”¨çš„åº¦é‡æœ‰ SQNRï¼ˆä¿¡å·é‡åŒ–å™ªå£°æ¯”ï¼Œå³ Signal to Quantized Noise Ratioï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMean-Squre-Errorï¼‰ã€‚è¿™ç§æ¯”è¾ƒåˆ†æä¹Ÿæœ‰åŠ©äºæŒ‡å¯¼è¿›ä¸€æ­¥çš„ä¼˜åŒ–ã€‚</p>
<p><img alt="" src="../../_images/compare_output_ns.png" /></p>
<p>PyTorch åœ¨æ•°å€¼å¥—ä»¶ä¸‹æä¾›äº†å¸®åŠ©è¿›è¡Œæ­¤åˆ†æçš„å·¥å…·ã€‚ä»å®Œæ•´çš„æ•™ç¨‹ä¸­äº†è§£æ›´å¤šå…³äºä½¿ç”¨ <a class="reference external" href="https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html">Numeric Suite</a> çš„ä¿¡æ¯ã€‚</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># extract from https://pytorch.org/tutorials/prototype/numeric_suite_tutorial.html</span>
<span class="kn">import</span> <span class="nn">torch.quantization._numeric_suite</span> <span class="k">as</span> <span class="nn">ns</span>

<span class="k">def</span> <span class="nf">SQNR</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Higher is better</span>
    <span class="n">Ps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">Pn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="mi">20</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">Ps</span><span class="o">/</span><span class="n">Pn</span><span class="p">)</span>

<span class="n">wt_compare_dict</span> <span class="o">=</span> <span class="n">ns</span><span class="o">.</span><span class="n">compare_weights</span><span class="p">(</span><span class="n">fp32_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">int8_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">wt_compare_dict</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">compute_error</span><span class="p">(</span><span class="n">wt_compare_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s1">&#39;float&#39;</span><span class="p">],</span> <span class="n">wt_compare_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s1">&#39;quantized&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()))</span>

<span class="n">act_compare_dict</span> <span class="o">=</span> <span class="n">ns</span><span class="o">.</span><span class="n">compare_model_outputs</span><span class="p">(</span><span class="n">fp32_model</span><span class="p">,</span> <span class="n">int8_model</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">act_compare_dict</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">compute_error</span><span class="p">(</span><span class="n">act_compare_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s1">&#39;float&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">act_compare_dict</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="s1">&#39;quantized&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dequantize</span><span class="p">()))</span>
</pre></div>
</div>
</section>
<section id="id16">
<h2>å¯¹æ‚¨å·¥ä½œæµç¨‹çš„å»ºè®®<a class="headerlink" href="#id16" title="æ°¸ä¹…é“¾æ¥è‡³æ ‡é¢˜">#</a></h2>
<p><img alt="" src="../../_images/quantization-flowchart2.png" /></p>
<p>è¦ç‚¹ï¼š</p>
<ul class="simple">
<li><p>å¤§ï¼ˆ10M+ å‚æ•°ï¼‰æ¨¡å‹å¯¹é‡åŒ–è¯¯å·®æ›´å…·é²æ£’æ€§ã€‚</p></li>
<li><p>ä» FP32 æ£€æŸ¥ç‚¹é‡åŒ–æ¨¡å‹æ¯”ä»é›¶å¼€å§‹è®­ç»ƒ INT8 æ¨¡å‹æä¾›äº†æ›´å¥½çš„ accuracyã€‚</p></li>
<li><p>åˆ†ææ¨¡å‹è¿è¡Œæ—¶æ˜¯å¯é€‰çš„ï¼Œä½†å®ƒå¯ä»¥å¸®åŠ©è¯†åˆ«é˜»ç¢æ¨ç†çš„å±‚ã€‚</p></li>
<li><p>åŠ¨æ€é‡åŒ–æ˜¯ä¸€ä¸ªç®€å•çš„ç¬¬ä¸€æ­¥ï¼Œç‰¹åˆ«æ˜¯å½“æ‚¨çš„æ¨¡å‹æœ‰è®¸å¤šçº¿æ€§æˆ–é€’å½’å±‚æ—¶ã€‚</p></li>
<li><p>ä½¿ç”¨é€é€šé“å¯¹ç§°é‡åŒ–å€Ÿç”± <code class="docutils literal notranslate"><span class="pre">MinMax</span></code> è§‚æµ‹è€…é‡åŒ–æƒé‡ã€‚ä½¿ç”¨é€å¼ é‡ä»¿å°„é‡åŒ–å€Ÿç”± <code class="docutils literal notranslate"><span class="pre">MovingAverageMinMax</span></code> è§‚æµ‹è€…é‡åŒ–æ¿€æ´»ã€‚</p></li>
<li><p>ä½¿ç”¨è¯¸å¦‚ SQNR ä¹‹ç±»çš„åº¦é‡æ¥ç¡®å®šå“ªäº›å±‚æœ€å®¹æ˜“å—åˆ°é‡åŒ–è¯¯å·®çš„å½±å“ã€‚å…³é—­è¿™äº›å±‚ä¸Šçš„é‡åŒ–ã€‚</p></li>
<li><p>ä½¿ç”¨ QAT å¯¹åŸå§‹è®­ç»ƒè°ƒåº¦çš„å¤§çº¦ <span class="math notranslate nohighlight">\(10\%\)</span> è¿›è¡Œå¾®è°ƒï¼Œé€€ç«å­¦ä¹ ç‡ï¼ˆannealing learning rateï¼‰è°ƒåº¦ä»åˆå§‹è®­ç»ƒå­¦ä¹ ç‡çš„ <span class="math notranslate nohighlight">\(1\%\)</span> å¼€å§‹ã€‚</p></li>
<li><p>å¦‚æœä¸Šé¢çš„å·¥ä½œæµç¨‹ä¸é€‚åˆä½ ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“æ›´å¤šã€‚å‘å¸ƒä¸€ä¸ªåŒ…å«ä½ çš„ä»£ç ç»†èŠ‚çš„å¸–å­ï¼ˆæ¨¡å‹æ¶æ„ï¼Œå‡†ç¡®æ€§æŒ‡æ ‡ï¼Œå°è¯•è¿‡çš„æŠ€æœ¯ï¼‰ã€‚è¯·æŠ„é€ <a class="reference external" href="https://discuss.pytorch.org/u/suraj.pt/">&#64;suraj.pt</a>ã€‚</p></li>
</ul>
</section>
</section>

<div class="section">
   
</div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="index.html" title="ä¸Šä¸€é¡µ é¡µ">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">ä¸Šä¸€é¡µ</p>
            <p class="prev-next-title">å¿«é€Ÿä¸Šæ‰‹</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="ns.html" title="ä¸‹ä¸€é¡µ é¡µ">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">ä¸‹ä¸€é¡µ</p>
        <p class="prev-next-title">Pytorch æ•°å€¼å¥—ä»¶æ•™ç¨‹</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By xinetzone<br/>
  
      &copy; Copyright 2021, xinetzone.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>