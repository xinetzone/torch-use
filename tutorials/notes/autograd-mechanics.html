
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>自动微分机制 &#8212; torch-book 0.0.1 文档</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/default.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "xinetzone/torch-book");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../_static/translations.js"></script>
    <script src="../../_static/design-tabs.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="canonical" href="https://xinetzone.github.io/torch-book/tutorials/notes/autograd-mechanics.html" />
    <link rel="shortcut icon" href="../../_static/favicon.jpg"/>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="目标检测" href="detection.html" />
    <link rel="prev" title="笔记" href="index.html" /> 
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
     
<link
  rel="alternate"
  type="application/atom+xml"
  href="../../posts/atom.xml"
  title="Blog"
/>
 
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.jpg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   项目简介
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../index.html">
   教程
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../basics/index.html">
     PyTorch 基础
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../basics/quickstart.html">
       快速入门
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../basics/autogradqs.html">
       自动微分
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../object-detection/index.html">
     目标检测
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../object-detection/yolo/index.html">
       YOLO 系列
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
      <label for="toctree-checkbox-4">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../object-detection/yolo/intro.html">
         YOLO 简介
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../object-detection/yolo/tutorials/index.html">
         YOLO 教程
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="index.html">
     笔记
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       自动微分机制
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="detection.html">
       目标检测
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="extending.html">
       扩展 PyTorch
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="thop.html">
       THOP: PyTorch OpCounter
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../seg-fine-tuning.html">
     微调分割
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../quant/index.html">
   量化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../quant/intro.html">
     量化简介
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../quant/start.html">
     快速入门
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../openmmlab/index.html">
   MMDetection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../openmmlab/start.html">
     MMDect 快速上手
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../chaos/index.html">
   Eager 量化(混乱)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../chaos/intro.html">
     量化简介（Eager）
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../chaos/recipes.html">
     量化菜谱
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../chaos/quantized-tensor.html">
     量化张量
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../chaos/start/index.html">
     快速上手
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
    <label for="toctree-checkbox-9">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../chaos/start/basic.html">
       基础
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../chaos/start/ns.html">
       Pytorch 数值套件教程
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../chaos/study/index.html">
     学习
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
    <label for="toctree-checkbox-10">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../chaos/study/intro.html">
       概述
      </a>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../chaos/study/transfer-learning/index.html">
       迁移学习
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
      <label for="toctree-checkbox-11">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/transfer-learning/basic.html">
         计算机视觉分类
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/transfer-learning/quantized.html">
         量化计算机视觉分类
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/transfer-learning/custom.html">
         自定义
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/transfer-learning/cifar.html">
         测试
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/transfer-learning/tvm.html">
         TVM
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../chaos/study/advanced/index.html">
       高级教程
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
      <label for="toctree-checkbox-12">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/advanced/custom.html">
         自定义量化
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/advanced/run.html">
         通用量化模型
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/advanced/qat.html">
         QAT
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../chaos/study/advanced/cifar.html">
         特定于 cifar10 的量化（待更）
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../chaos/study/qat-resnet18.html">
       QAT（resnet18）
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../chaos/study/test.html">
       测试 QAT
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../chaos/study/draft.html">
       回收站
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../chaos/tutorial/index.html">
     教程
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="simple">
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../chaos/papers/index.html">
     论文
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
    <label for="toctree-checkbox-14">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../chaos/papers/gholami2021survey.html">
       A Survey of Quantization Methods for Efficient Neural Network Inference
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ecosystem/index.html">
   生态系统
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ecosystem/intro.html">
     简介
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../others/index.html">
   其他
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../others/colab.html">
     Colab 训练
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../others/en-zh.html">
     中英互译
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../others/raw.html">
     未整理的资料
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../refs.html">
   参考
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../glossary/index.html">
   术语表
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../glossary/numpy.html">
     NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../glossary/pytorch.html">
     PyTorch 词汇表
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <div>
版权所有 © 2021 <a href="https://xinetzone.github.io/">xinetzone</a></div>
<div>由 <a href="https://ebp.jupyterbook.org/">EBP</a> 提供技术支持</div>
<a href="https://torch-book.readthedocs.io/">版本切换</a>

            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/xinetzone/torch-book/main?urlpath=lab/tree/docs/tutorials/notes/autograd-mechanics.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/xinetzone/torch-book/blob/main/docs/tutorials/notes/autograd-mechanics.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/xinetzone/torch-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/xinetzone/torch-book/issues/new?title=Issue%20on%20page%20%2Ftutorials/notes/autograd-mechanics.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/xinetzone/torch-book/edit/main/docs/tutorials/notes/autograd-mechanics.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/tutorials/notes/autograd-mechanics.ipynb.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> 导航
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#autograd">
   <code class="docutils literal notranslate">
    <span class="pre">
     autograd
    </span>
   </code>
   如何编码历史
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     已保存的张量
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   局部禁用梯度计算
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#requires-grad">
     设置
     <code class="docutils literal notranslate">
      <span class="pre">
       requires_grad
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     模式
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   使用 autograd 进行就地操作
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     就地正确性检查
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   多线程 Autograd
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   复数 Autograd
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   保存张量的挂钩
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     注册已保存张量的钩子
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     为保存的张量注册默认钩子
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>自动微分机制</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> 导航 </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#autograd">
   <code class="docutils literal notranslate">
    <span class="pre">
     autograd
    </span>
   </code>
   如何编码历史
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     已保存的张量
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   局部禁用梯度计算
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#requires-grad">
     设置
     <code class="docutils literal notranslate">
      <span class="pre">
       requires_grad
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     模式
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   使用 autograd 进行就地操作
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     就地正确性检查
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   多线程 Autograd
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   复数 Autograd
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   保存张量的挂钩
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     注册已保存张量的钩子
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     为保存的张量注册默认钩子
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                 <section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>自动微分机制<a class="headerlink" href="#id1" title="永久链接至标题">#</a></h1>
<p><span class="guilabel">参考</span>：<a class="reference external" href="https://pytorch.org/docs/stable/notes/autograd.html">notes/autograd</a></p>
<p>本文将概述 <code class="docutils literal notranslate"><span class="pre">autograd</span></code> 的工作方式并记录运算。完全没有必要理解所有这些，但建议您熟悉它，因为它将帮助您编写更有效、更干净的程序，并可以帮助您调试。</p>
<section id="autograd">
<h2><code class="docutils literal notranslate"><span class="pre">autograd</span></code> 如何编码历史<a class="headerlink" href="#autograd" title="永久链接至标题">#</a></h2>
<p>Autograd 是反向自动微分系统。从概念上讲，<code class="docutils literal notranslate"><span class="pre">autograd</span></code> 记录了图，该图记录了在执行运算时创建数据的所有运算，从而为您提供了一个有向无环图，其叶是输入张量，根是输出张量。通过从根到叶跟踪这个图，可以使用链式法则自动计算梯度。</p>
<p>在内部，<code class="docutils literal notranslate"><span class="pre">autograd</span></code> 将这个图表示为 <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(在 PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> 对象（实际上是表达式）的图，可以借助 <code class="xref py py-func docutils literal notranslate"><span class="pre">apply()</span></code> 来计算对图求值的结果。在计算 forward 传递时，<code class="docutils literal notranslate"><span class="pre">autograd</span></code> 同时申请计算，并构建表示计算梯度（每个 <a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(在 PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> 的 <code class="docutils literal notranslate"><span class="pre">.grad_fn</span></code> 属性是这个图的入口点)。当前向传递完成时，在向后传递中计算这个图的梯度。</p>
<div class="admonition important">
<p class="admonition-title">重要</p>
<p>图在每次迭代时都是从头开始重新创建的，这正是允许使用任意 Python 控制流语句的原因，这些语句可以在每次迭代时改变图的总体形状和大小。在启动训练之前，您不必对所有可能的路径进行编码——您所运行的是您所区分的。</p>
</div>
<section id="id2">
<h3>已保存的张量<a class="headerlink" href="#id2" title="永久链接至标题">#</a></h3>
<p>有些运算需要在向前传播期间保存中间结果，以便执行 backward 传播。例如，函数 <span class="math notranslate nohighlight">\(x\mapsto x^2\)</span> 保存输入的 <span class="math notranslate nohighlight">\(x\)</span> 来计算梯度。</p>
<p>当定义自定义 Python 函数时，你可以使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">save_for_backward()</span></code> 在正向传播时保存张量，在后向传播时使用 <code class="docutils literal notranslate"><span class="pre">saved_tensor</span></code> 来检索它们。有关更多信息，请参见 <a class="reference internal" href="extending.html"><span class="doc std std-doc">扩展 PyTorch</span></a>。</p>
<p>对于 PyTorch 定义的运算（如 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow" title="(在 PyTorch v1.12)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.pow()</span></code></a>），张量会根据需要自动保存。通过查找以 <code class="docutils literal notranslate"><span class="pre">_saved</span></code> 前缀开头的属性，您可以研究（出于教育或调试目的）某个 <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> 保存了哪些张量。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_saved_self</span><span class="p">))</span>  <span class="c1"># True</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span> <span class="ow">is</span> <span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_saved_self</span><span class="p">)</span>  <span class="c1"># True</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
True
</pre></div>
</div>
</div>
</div>
<p>在前面的代码中，<code class="docutils literal notranslate"><span class="pre">y.grad_fn._saved_self</span></code> 指的是与 <code class="docutils literal notranslate"><span class="pre">x</span></code> 相同的张量对象。但情况并非总是如此。例如</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_saved_result</span><span class="p">))</span>  <span class="c1"># True</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span> <span class="ow">is</span> <span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_saved_result</span><span class="p">)</span>  <span class="c1"># False</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
False
</pre></div>
</div>
</div>
</div>
<p>在幕后，为了防止循环引用，PyTorch 在保存时将张量打包，并将其解压到不同的张量以供阅读。在这里，你通过访问 <code class="docutils literal notranslate"><span class="pre">y.grad_fn._saved_result</span></code> 得到的张量是不同于 <code class="docutils literal notranslate"><span class="pre">y</span></code> 的张量对象（但它们仍然共享相同的存储）。</p>
<p>张量是否会被打包到不同的张量对象中取决于它是否是它自己的 <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> 的输出，这是可能会改变的实现细节，用户不应该依赖它。</p>
<p>你可以控制 PyTorch 如何借助<a class="reference external" href="https://pytorch.org/docs/stable/notes/autograd.html#saved-tensors-hooks-doc">保存张量的挂钩</a> 打包/拆包。</p>
</section>
</section>
<section id="id3">
<h2>局部禁用梯度计算<a class="headerlink" href="#id3" title="永久链接至标题">#</a></h2>
<p>Python 中有几种机制可以在局部禁用梯度计算：</p>
<p>要在整个代码块中禁用梯度，有一些上下文管理器，如 <code class="docutils literal notranslate"><span class="pre">no-grad</span></code> 模式和 <code class="docutils literal notranslate"><span class="pre">inference</span></code> 模式。为了从梯度计算中更细粒度地排除子图，可以设置张量的 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 字段。</p>
<p>下面，除了讨论上面的机制，还描述了求值模式（<code class="xref py py-func docutils literal notranslate"><span class="pre">eval()</span></code>），这个方法实际上并不用于禁用梯度计算，但由于它的名称，经常与这三种方法混在一起。</p>
<section id="requires-grad">
<h3>设置 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code><a class="headerlink" href="#requires-grad" title="永久链接至标题">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 是 flag，除非用 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="(在 PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code></a> 包装，否则默认为 <code class="docutils literal notranslate"><span class="pre">False</span></code>，它允许从梯度计算中细粒度地排除子图。它在向前和向后的传播中都起作用：</p>
<p>在前向传播过程中，如果运算至少有一个输入张量需要梯度，则该运算只记录在 backward 图中。在后向传播（<code class="docutils literal notranslate"><span class="pre">.backward()</span></code>）期间，只有 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 的叶张量才会在它们的 <code class="docutils literal notranslate"><span class="pre">.grad</span></code> 字段中累积梯度。</p>
<p>值得注意的是，即使每个张量都有这个 flag，设置它只对叶张量有意义（没有 <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> 的张量，例如 <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> 参数）。非叶张量（确实有 <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> 的张量）是具有 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 图关联的张量。因此，它们的梯度将需要作为中间结果来计算需要梯度的叶张量的梯度。从这个定义中，很明显，所有非叶张量都将自动具有 <code class="docutils literal notranslate"><span class="pre">require_grad=True</span></code>。</p>
<p>设置 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 应该是您控制模型的哪些部分是梯度计算的一部分的主要方式，例如，如果您需要在模型微调期间冻结您的预训练模型的部分。</p>
<p>要冻结模型的部分，只需将 <code class="docutils literal notranslate"><span class="pre">.requires_grad_(False)</span></code> 应用于不希望更新的参数。如上所述，由于使用这些参数作为输入的计算不会在正向传递中被记录，因此它们的 <code class="docutils literal notranslate"><span class="pre">.grad</span></code> 字段不会在向后传递中被更新，因为它们一开始就不是 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 传递图的一部分，正如所希望的那样。</p>
<p>因为这是非常常见的模式，所以也可以在模块级别使用 <code class="docutils literal notranslate"><span class="pre">nn.Module.requires_grad_()</span></code> 来设置 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code>。当应用到模块时，<code class="docutils literal notranslate"><span class="pre">.requires_grad_()</span></code> 将对模块的所有参数（默认为 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>）生效。</p>
</section>
<section id="id4">
<h3>模式<a class="headerlink" href="#id4" title="永久链接至标题">#</a></h3>
<p>除了设置 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 之外，Python 还支持三种可能的模式，它们可以影响内部 <code class="docutils literal notranslate"><span class="pre">autograd</span></code> 处理 PyTorch 中的计算：default 模式（grad 模式）、no-grad 模式和 <code class="docutils literal notranslate"><span class="pre">inference</span></code> 模式，所有这些模式都可以通过上下文管理器和装饰器进行切换。</p>
<p class="rubric">默认模式（Grad 模式）</p>
<p>默认模式实际上就是当没有启用其他模式（如 no-grad 模式和 inference 模式）时，隐式地处于的模式。与 no-grad 模式相比，默认模式有时也称为 grad模式。</p>
<p>关于默认模式最重要的一点是，它是 <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 生效的唯一模式。在其他两种模式中，<code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> 总是被重写为 <code class="docutils literal notranslate"><span class="pre">False</span></code>。</p>
<p class="rubric">No-grad 模式</p>
<p>在 no-grad 模式下的计算表现为没有任何输入需要 grad。换句话说，即使有 <code class="docutils literal notranslate"><span class="pre">require_grad=True</span></code> 的输入，在 no-grad 模式下的计算也不会记录在反向图中。</p>
<p>当您需要执行不应由 <code class="docutils literal notranslate"><span class="pre">autograd</span></code> 记录的运算，但您仍然希望稍后在 <code class="docutils literal notranslate"><span class="pre">grad</span></code> 模式下使用这些计算的输出时，启用 <code class="docutils literal notranslate"><span class="pre">no-grad</span></code> 模式。这个上下文管理器可以方便地禁用代码块或函数的梯度，而不必临时将张量设置为 <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>，然后返回 <code class="docutils literal notranslate"><span class="pre">True</span></code>。</p>
<p>例如，在编写优化器时，no-grad 模式可能很有用：在执行训练更新时，您希望在原地更新参数，而不需要由 autograd 记录更新。您还打算在下一个前向传递中在 grad 模式中使用更新的参数进行计算。</p>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn.init</span></code> 中的实现在初始化参数时也依赖于 no-grad 模式，以避免在原地更新初始化参数时进行自 grad 跟踪。</p>
<p class="rubric">inference 模式</p>
<p>推理模式是 no-grad 模式的极端版本。就像在 no-grad 模式中一样，推理模式中的计算不会记录在反向图中，但是启用推理模式将允许 PyTorch 进一步加速您的模型。这种更好的运行时有一个缺点：在推理模式中创建的张量不能用于退出推理模式后由 autograd 记录的计算。</p>
<p>当您执行不需要记录在反向图中的计算时，启用推理模式，并且您不打算在稍后由 autograd 记录的任何计算中使用推理模式中创建的张量。</p>
<p>建议您在不需要自动跟踪（例如，数据处理和模型评估）的代码部分尝试推理模式。如果它在你的用例中是开箱即用的，这是免费的性能胜利。如果在启用推理模式后遇到错误，请检查是否在退出推理模式后由 autograd 记录的计算中使用了推理模式中创建的张量。如果您无法避免在您的情况下使用这种方法，您总是可以切换回 no-grad 模式。</p>
<p>有关推理模式的详细信息，请参见 <a class="reference external" href="https://pytorch.org/cppdocs/notes/inference_mode.html">推理模式</a>。</p>
<p>有关推理模式的实现细节，请参见 <a class="reference external" href="https://github.com/pytorch/rfcs/pull/17">RFC-0011-InferenceMode</a>。</p>
<p class="rubric">评估模式</p>
<p>评估模式（<code class="docutils literal notranslate"><span class="pre">nn.Module.eval</span></code>）实际上并不是一种局部禁用梯度计算的机制。无论如何，这里包含了它，因为它有时会被混淆为这样的机制。</p>
<p>在函数上，<code class="xref py py-func docutils literal notranslate"><span class="pre">module.eval()</span></code>（或等效于 <code class="xref py py-func docutils literal notranslate"><span class="pre">module.train()</span></code>）完全正交于 no-grade 模式和推理模式。e<code class="xref py py-func docutils literal notranslate"><span class="pre">module.eval()</span></code> 如何影响您的模型完全取决于您的模型中使用的特定模块，以及它们是否定义了任何训练模式特定的行为。</p>
<p>如果你的模型依赖于诸如 <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout" title="(在 PyTorch v1.12)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Dropout</span></code></a> 和 <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.BatchNorm2d</span> </code> 这样的模块，你需要负责调用 <code class="xref py py-func docutils literal notranslate"><span class="pre">model.eval()</span></code> 和 <code class="xref py py-func docutils literal notranslate"><span class="pre">model.train()</span></code>。根据训练模式的不同，<code class="docutils literal notranslate"><span class="pre">BatchNorm2d</span></code> 的行为可能会有所不同，例如，在验证数据上避免更新 <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> 运行统计数据。</p>
<div class="admonition- admonition">
<p class="admonition-title">建议</p>
<p>在训练时使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">model.train()</span></code>，在评估模型（验证/测试）时使用 <code class="xref py py-func docutils literal notranslate"><span class="pre">model.eval()</span></code>，即使你不确定你的模型有特定的训练模式行为，因为你正在使用的模块可能会在 training 和 eval 模式中被更新为不同的行为。</p>
</div>
</section>
</section>
<section id="id5">
<h2>使用 autograd 进行就地操作<a class="headerlink" href="#id5" title="永久链接至标题">#</a></h2>
<p>在 autograd 中支持就地操作（in-place）是一件困难的事情，不鼓励在大多数情况下使用它们。Autograd 积极的缓冲区释放和重用使其非常高效，而且很少有情况下，就地操作实际上显著降低了内存使用量。除非您在沉重的内存压力下操作，否则您可能永远都不需要使用它们。</p>
<p>有两个主要原因限制了就地操作的适用性：</p>
<ol class="arabic simple">
<li><p>就地操作可能会覆盖计算梯度所需的值。</p></li>
<li><p>每个就地操作实际上都需要实现重写计算图。错位（out-of-place）版本只是分配新对象并保持对旧图形的引用，而就地操作则需要更改表示此操作的函数的所有输入的创建者。这可能很棘手，特别是当有很多张量引用了相同的存储（例如通过索引或转置创建），如果修改后的输入的存储被其他张量引用，原地函数实际上会引发错误。</p></li>
</ol>
<section id="id6">
<h3>就地正确性检查<a class="headerlink" href="#id6" title="永久链接至标题">#</a></h3>
<p>每个张量都有版本计数器，每当它在任何操作中被标记为 dirty 时，这个计数器就会递增。当函数为 backward, 保存任何张量时，包含它们的张量的版本计数器也会被保存。一旦你访问 <code class="docutils literal notranslate"><span class="pre">self.saved_tensors</span></code> 会检查它，如果它大于保存的值，则会引发错误。这确保了如果您使用的是就地函数而没有看到任何错误，您可以确保计算的梯度是正确的。</p>
</section>
</section>
<section id="id7">
<h2>多线程 Autograd<a class="headerlink" href="#id7" title="永久链接至标题">#</a></h2>
<p>autograd 引擎负责运行计算 backward 遍历所需的所有 backward 运算。本节将描述所有可以帮助您在多线程环境中充分利用它的细节。（这只与 PyTorch 1.6+ 相关，因为之前版本的行为是不同的）。</p>
<p>用户可以用多线程代码训练他们的模型（例如 Hogwild 训练），并且不会阻塞并发 backward 计算，示例代码：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a train function to be used in different threads</span>
<span class="k">def</span> <span class="nf">train_fn</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># forward</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
    <span class="c1"># backward</span>
    <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># potential optimizer update</span>


<span class="c1"># User write their own threading code to drive the train_fn</span>
<span class="n">threads</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">train_fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">())</span>
    <span class="n">p</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">threads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">threads</span><span class="p">:</span>
    <span class="n">p</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
<p>请注意用户应该注意的一些行为：</p>
<p class="rubric">CPU 的并发性</p>
<p>当你通过 Python 或 C++ API 在 CPU 上的多个线程中运行 <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code> 或 <code class="xref py py-func docutils literal notranslate"><span class="pre">grad()</span></code> 时，你希望看到额外的并发性，而不是在执行期间以特定的顺序序列化所有的 backward 调用（在 PyTorch 1.6 之前的行为）。</p>
<p class="rubric">非确定性</p>
<p>如果你在多线程上并发地调用 <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code>，但是有共享的输入（例如 Hogwild CPU 训练）。因为参数是在线程之间自动共享的，所以梯度累加在线程之间的 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 调用上可能变得不确定，因为两个 <code class="docutils literal notranslate"><span class="pre">backward</span></code> 调用可能访问并试图累积相同的 <code class="docutils literal notranslate"><span class="pre">.grad</span></code> 属性。这在技术上是不安全的，它可能会导致竞争条件和结果可能是无效的使用。</p>
<p>但是，如果您使用多线程方法来驱动整个训练过程，但是使用共享参数，那么这是预期的模式，使用多线程的用户应该记住线程模型，并预期会发生这种情况。用户可以使用函数式 API <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad" title="(在 PyTorch v1.12)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a> 来计算梯度，而不是 <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code> 来避免不确定性。</p>
<p class="rubric">Graph retaining</p>
<p>如果 autograd 图的一部分在线程之间共享，也就是说，运行前向单线程的第一部分，然后在多个线程中运行第二部分，那么图的第一部分就是共享的。在这种情况下，在同一个图上执行 <code class="xref py py-func docutils literal notranslate"><span class="pre">grad()</span></code> 或 <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code> 的不同线程可能会在一个线程上破坏图，而在这种情况下，另一个线程将崩溃。Autograd 将向用户输出错误信息，类似于在 <code class="docutils literal notranslate"><span class="pre">retain_graph=True</span></code> 的情况下调用 <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code> 两次，并让用户知道他们应该使用 <code class="docutils literal notranslate"><span class="pre">retain_graph=True</span></code>。</p>
<p class="rubric">Autograd 节点上的线程安全</p>
<p>由于 Autograd 允许调用方线程驱动 backward 执行以获得潜在的并行性，因此确保 CPU 上的线程安全是很重要的，向后并行共享部分/全部的 GraphTask。</p>
<p>自定义Python <code class="docutils literal notranslate"><span class="pre">autograd.function</span></code> 是自动线程安全的，因为 GIL。对于内置的 C++ Autograd 节点（例如 AccumulateGrad, CopySlices） 和自定义的 <code class="docutils literal notranslate"><span class="pre">autograd::Function</span></code>，Autograd Engine 使用线程互斥锁来保护可能有状态写/读的 autograd 节点上的线程安全。</p>
<p class="rubric">C++ 钩子上没有线程安全</p>
<p>Autograd 依赖于用户来编写线程安全的 C++ 钩子。如果你想在多线程环境中正确的应用钩子，你需要写正确的线程锁定代码来确保钩子是线程安全的。</p>
</section>
<section id="id8">
<h2>复数 Autograd<a class="headerlink" href="#id8" title="永久链接至标题">#</a></h2>
<p>简短的版本：</p>
<ul class="simple">
<li><p>When you use PyTorch to differentiate any function <span class="math notranslate nohighlight">\(f(z)\)</span> with complex domain and/or codomain,
the gradients are computed under the assumption that the function is a part of a larger real-valued
loss function <span class="math notranslate nohighlight">\(g(input)=L\)</span>. The gradient computed is <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial z^*}\)</span>
(note the conjugation of z), the negative of which is precisely the direction of steepest descent
used in Gradient Descent algorithm. Thus, all the existing optimizers work out of
the box with complex parameters.</p></li>
<li><p>This convention matches TensorFlow’s convention for complex
differentiation, but is different from JAX (which computes
<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial z}\)</span>).</p></li>
<li><p>If you have a real-to-real function which internally uses complex
operations, the convention here doesn’t matter: you will always get
the same result that you would have gotten if it had been implemented
with only real operations.</p></li>
</ul>
<p>If you are curious about the mathematical details, or want to know how
to define complex derivatives in PyTorch, read on.</p>
<p class="rubric">What are complex derivatives?</p>
<p>The mathematical definition of complex-differentiability takes the
limit definition of a derivative and generalizes it to operate on
complex numbers. Consider a function <span class="math notranslate nohighlight">\(f: ℂ → ℂ\)</span>,</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[`f(z=x+yj) = u(x, y) + v(x, y)j`\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> are two variable real valued functions.</p>
<p>Using the derivative definition, we can write:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[f'(z) = \lim_{h \to 0, h \in C} \frac{f(z+h) - f(z)}{h}\]</div>
</div></blockquote>
<p>In order for this limit to exist, not only must <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> must be
real differentiable, but <span class="math notranslate nohighlight">\(f\)</span> must also satisfy the Cauchy-Riemann <a class="reference external" href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations">equations</a>.  In
other words: the limit computed for real and imaginary steps (<span class="math notranslate nohighlight">\(h\)</span>)
must be equal. This is a more restrictive condition.</p>
<p>The complex differentiable functions are commonly known as holomorphic
functions. They are well behaved, have all the nice properties that
you’ve seen from real differentiable functions, but are practically of no
use in the optimization world. For optimization problems, only real valued objective
functions are used in the research community since complex numbers are not part of any
ordered field and so having complex valued loss does not make much sense.</p>
<p>It also turns out that no interesting real-valued objective fulfill the
Cauchy-Riemann equations. So the theory with homomorphic function cannot be
used for optimization and most people therefore use the Wirtinger calculus.</p>
<p class="rubric">Wirtinger Calculus comes in picture …</p>
<p>So, we have this great theory of complex differentiability and
holomorphic functions, and we can’t use any of it at all, because many
of the commonly used functions are not holomorphic. What’s a poor
mathematician to do? Well, Wirtinger observed that even if <span class="math notranslate nohighlight">\(f(z)\)</span>
isn’t holomorphic, one could rewrite it as a two variable function
<span class="math notranslate nohighlight">\(f(z, z*)\)</span> which is always holomorphic. This is because real and
imaginary of the components of <span class="math notranslate nohighlight">\(z\)</span> can be expressed in terms of
<span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(z^*\)</span> as:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    Re(z) &amp;= \frac {z + z^*}{2} \\
    Im(z) &amp;= \frac {z - z^*}{2j}
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>Wirtinger calculus suggests to study <span class="math notranslate nohighlight">\(f(z, z^*)\)</span> instead, which is
guaranteed to be holomorphic if <span class="math notranslate nohighlight">\(f\)</span> was real differentiable (another
way to think of it is as a change of coordinate system, from <span class="math notranslate nohighlight">\(f(x, y)\)</span>
to <span class="math notranslate nohighlight">\(f(z, z^*)\)</span>.)  This function has partial derivatives
<span class="math notranslate nohighlight">\(\frac{\partial }{\partial z}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial}{\partial z^{*}}\)</span>.
We can use the chain rule to establish a
relationship between these partial derivatives and the partial
derivatives w.r.t., the real and imaginary components of <span class="math notranslate nohighlight">\(z\)</span>.</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \frac{\partial }{\partial x} &amp;= \frac{\partial z}{\partial x} * \frac{\partial }{\partial z} + \frac{\partial z^*}{\partial x} * \frac{\partial }{\partial z^*} \\
                                 &amp;= \frac{\partial }{\partial z} + \frac{\partial }{\partial z^*}   \\
    \\
    \frac{\partial }{\partial y} &amp;= \frac{\partial z}{\partial y} * \frac{\partial }{\partial z} + \frac{\partial z^*}{\partial y} * \frac{\partial }{\partial z^*} \\
                                 &amp;= 1j * (\frac{\partial }{\partial z} - \frac{\partial }{\partial z^*})
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>From the above equations, we get:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \frac{\partial }{\partial z} &amp;= 1/2 * (\frac{\partial }{\partial x} - 1j * \frac{\partial }{\partial y})   \\
    \frac{\partial }{\partial z^*} &amp;= 1/2 * (\frac{\partial }{\partial x} + 1j * \frac{\partial }{\partial y})
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>which is the classic definition of Wirtinger calculus that you would find on <a class="reference external" href="https://en.wikipedia.org/wiki/Wirtinger_derivatives">Wikipedia</a>.</p>
<p>There are a lot of beautiful consequences of this change.</p>
<ul class="simple">
<li><p>For one, the Cauchy-Riemann equations translate into simply saying that <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial z^*} = 0\)</span> (that is to say, the function <span class="math notranslate nohighlight">\(f\)</span> can be written
entirely in terms of <span class="math notranslate nohighlight">\(z\)</span>, without making reference to <span class="math notranslate nohighlight">\(z^*\)</span>).</p></li>
<li><p>Another important (and somewhat counterintuitive) result, as we’ll see later, is that when we do optimization on a real-valued loss, the step we should
take while making variable update is given by <span class="math notranslate nohighlight">\(\frac{\partial Loss}{\partial z^*}\)</span> (not <span class="math notranslate nohighlight">\(\frac{\partial Loss}{\partial z}\)</span>).</p></li>
</ul>
<p>For more reading, check out: <a class="reference external" href="https://arxiv.org/pdf/0906.4835.pdf">https://arxiv.org/pdf/0906.4835.pdf</a></p>
<p class="rubric">How is Wirtinger Calculus useful in optimization?</p>
<p>Researchers in audio and other fields, more commonly, use gradient
descent to optimize real valued loss functions with complex variables.
Typically, these people treat the real and imaginary values as separate
channels that can be updated. For a step size <span class="math notranslate nohighlight">\(\alpha/2\)</span> and loss
<span class="math notranslate nohighlight">\(L\)</span>, we can write the following equations in <span class="math notranslate nohighlight">\(ℝ^2\)</span>:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    x_{n+1} &amp;= x_n - (\alpha/2) * \frac{\partial L}{\partial x}  \\
    y_{n+1} &amp;= y_n - (\alpha/2) * \frac{\partial L}{\partial y}
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>How do these equations translate into complex space <span class="math notranslate nohighlight">\(ℂ\)</span>?</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    z_{n+1} &amp;= x_n - (\alpha/2) * \frac{\partial L}{\partial x} + 1j * (y_n - (\alpha/2) * \frac{\partial L}{\partial y}) \\
            &amp;= z_n - \alpha * 1/2 * (\frac{\partial L}{\partial x} + j \frac{\partial L}{\partial y}) \\
            &amp;= z_n - \alpha * \frac{\partial L}{\partial z^*}
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>Something very interesting has happened: Wirtinger calculus tells us
that we can simplify the complex variable update formula above to only
refer to the conjugate Wirtinger derivative
<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial z^*}\)</span>, giving us exactly the step we take in optimization.</p>
<p>Because the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative
when you differentiate a function with a real valued loss.</p>
<p class="rubric">How does PyTorch compute the conjugate Wirtinger derivative?</p>
<p>Typically, our derivative formulas take in <cite>grad_output</cite> as an input,
representing the incoming Vector-Jacobian product that we’ve already
computed, aka, <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial s^*}\)</span>, where <span class="math notranslate nohighlight">\(L\)</span>
is the loss of the entire computation (producing a real loss) and
<span class="math notranslate nohighlight">\(s\)</span> is the output of our function. The goal here is to compute
<span class="math notranslate nohighlight">\(\frac{\partial L}{\partial z^*}\)</span>, where <span class="math notranslate nohighlight">\(z\)</span> is the input of
the function.  It turns out that in the case of real loss, we can
get away with <em>only</em> calculating <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial z^*}\)</span>,
even though the chain rule implies that we also need to
have access to <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial z^*}\)</span>.  If you want
to skip this derivation, look at the last equation in this section
and then skip to the next section.</p>
<p>Let’s continue working with <span class="math notranslate nohighlight">\(f: ℂ → ℂ\)</span> defined as
<span class="math notranslate nohighlight">\(f(z) = f(x+yj) = u(x, y) + v(x, y)j\)</span>. As discussed above,
autograd’s gradient convention is centered around optimization for real
valued loss functions, so let’s assume <span class="math notranslate nohighlight">\(f\)</span> is a part of larger
real valued loss function <span class="math notranslate nohighlight">\(g\)</span>. Using chain rule, we can write:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-1">
<span class="eqno">(1)<a class="headerlink" href="#equation-1" title="公式的永久链接">#</a></span>\[\frac{\partial L}{\partial z^*} = \frac{\partial L}{\partial u} * \frac{\partial u}{\partial z^*} + \frac{\partial L}{\partial v} * \frac{\partial v}{\partial z^*}\]</div>
</div></blockquote>
<p>Now using Wirtinger derivative definition, we can write:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \frac{\partial L}{\partial s} = 1/2 * (\frac{\partial L}{\partial u} - \frac{\partial L}{\partial v} j) \\
    \frac{\partial L}{\partial s^*} = 1/2 * (\frac{\partial L}{\partial u} + \frac{\partial L}{\partial v} j)
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>It should be noted here that since <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(v\)</span> are real
functions, and <span class="math notranslate nohighlight">\(L\)</span> is real by our assumption that <span class="math notranslate nohighlight">\(f\)</span> is a
part of a real valued function, we have:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-2">
<span class="eqno">(2)<a class="headerlink" href="#equation-2" title="公式的永久链接">#</a></span>\[(\frac{\partial L}{\partial s})^* = \frac{\partial L}{\partial s^*}\]</div>
</div></blockquote>
<p>i.e., <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial s}\)</span> equals to <span class="math notranslate nohighlight">\(grad\_output^*\)</span>.</p>
<p>Solving the above equations for <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial u}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial v}\)</span>, we get:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-3">
<span class="eqno">(3)<a class="headerlink" href="#equation-3" title="公式的永久链接">#</a></span>\[\begin{split}\begin{aligned}
    \frac{\partial L}{\partial u} = \frac{\partial L}{\partial s} + \frac{\partial L}{\partial s^*} \\
    \frac{\partial L}{\partial v} = -1j * (\frac{\partial L}{\partial s} - \frac{\partial L}{\partial s^*})
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>Substituting <a class="reference internal" href="#equation-3">(3)</a> in <a class="reference internal" href="#equation-1">(1)</a>, we get:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \frac{\partial L}{\partial z^*} &amp;= (\frac{\partial L}{\partial s} + \frac{\partial L}{\partial s^*}) * \frac{\partial u}{\partial z^*} - 1j * (\frac{\partial L}{\partial s} - \frac{\partial L}{\partial s^*}) * \frac{\partial v}{\partial z^*}  \\
                                    &amp;= \frac{\partial L}{\partial s} * (\frac{\partial u}{\partial z^*} + \frac{\partial v}{\partial z^*} j) + \frac{\partial L}{\partial s^*} * (\frac{\partial u}{\partial z^*} - \frac{\partial v}{\partial z^*} j)  \\
                                    &amp;= \frac{\partial L}{\partial s^*} * \frac{\partial (u + vj)}{\partial z^*} + \frac{\partial L}{\partial s} * \frac{\partial (u + vj)^*}{\partial z^*}  \\
                                    &amp;= \frac{\partial L}{\partial s} * \frac{\partial s}{\partial z^*} + \frac{\partial L}{\partial s^*} * \frac{\partial s^*}{\partial z^*}    \\
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>Using <a class="reference internal" href="#equation-2">(2)</a>, we get:</p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-4">
<span class="eqno">(4)<a class="headerlink" href="#equation-4" title="公式的永久链接">#</a></span>\[\begin{split}\begin{aligned}
    \frac{\partial L}{\partial z^*} &amp;= (\frac{\partial L}{\partial s^*})^* * \frac{\partial s}{\partial z^*} + \frac{\partial L}{\partial s^*} * (\frac{\partial s}{\partial z})^*  \\
                                    &amp;= \boxed{ (grad\_output)^* * \frac{\partial s}{\partial z^*} + grad\_output * {(\frac{\partial s}{\partial z})}^* }       \\
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>This last equation is the important one for writing your own gradients,
as it decomposes our derivative formula into a simpler one that is easy
to compute by hand.</p>
<p class="rubric">How can I write my own derivative formula for a complex function?</p>
<p>The above boxed equation gives us the general formula for all
derivatives on complex functions.  However, we still need to
compute <span class="math notranslate nohighlight">\(\frac{\partial s}{\partial z}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial s}{\partial z^*}\)</span>.
There are two ways you could do this:</p>
<blockquote>
<div><ul class="simple">
<li><p>The first way is to just use the definition of Wirtinger derivatives directly and calculate <span class="math notranslate nohighlight">\(\frac{\partial s}{\partial z}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial s}{\partial z^*}\)</span> by
using <span class="math notranslate nohighlight">\(\frac{\partial s}{\partial x}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial s}{\partial y}\)</span>
(which you can compute in the normal way).</p></li>
<li><p>The second way is to use the change of variables trick and rewrite <span class="math notranslate nohighlight">\(f(z)\)</span> as a two variable function <span class="math notranslate nohighlight">\(f(z, z^*)\)</span>, and compute
the conjugate Wirtinger derivatives by treating <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(z^*\)</span> as independent variables. This is often easier; for example, if the function in question is holomorphic, only <span class="math notranslate nohighlight">\(z\)</span> will be used (and <span class="math notranslate nohighlight">\(\frac{\partial s}{\partial z^*}\)</span> will be zero).</p></li>
</ul>
</div></blockquote>
<p>Let’s consider the function <span class="math notranslate nohighlight">\(f(z = x + yj) = c * z = c * (x+yj)\)</span> as an example, where <span class="math notranslate nohighlight">\(c \in ℝ\)</span>.</p>
<p>Using the first way to compute the Wirtinger derivatives, we have.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \frac{\partial s}{\partial z} &amp;= 1/2 * (\frac{\partial s}{\partial x} - \frac{\partial s}{\partial y} j) \\
                                  &amp;= 1/2 * (c - (c * 1j) * 1j)  \\
                                  &amp;= c                          \\
    \\
    \\
    \frac{\partial s}{\partial z^*} &amp;= 1/2 * (\frac{\partial s}{\partial x} + \frac{\partial s}{\partial y} j) \\
                                    &amp;= 1/2 * (c + (c * 1j) * 1j)  \\
                                    &amp;= 0                          \\
\end{aligned}\end{split}\]</div>
<p>Using <a class="reference internal" href="#equation-4">(4)</a>, and <cite>grad_output = 1.0</cite> (which is the default grad output value used when <code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code> is called on a scalar output in PyTorch), we get:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z^*} = 1 * 0 + 1 * c = c\]</div>
</div></blockquote>
<p>Using the second way to compute Wirtinger derivatives, we directly get:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
   \frac{\partial s}{\partial z} &amp;= \frac{\partial (c*z)}{\partial z}       \\
                                 &amp;= c                                       \\
    \frac{\partial s}{\partial z^*} &amp;= \frac{\partial (c*z)}{\partial z^*}       \\
                                 &amp;= 0
\end{aligned}\end{split}\]</div>
</div></blockquote>
<p>And using <a class="reference internal" href="#equation-4">(4)</a> again, we get <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial z^*} = c\)</span>. As you can see, the second way involves lesser calculations, and comes
in more handy for faster calculations.</p>
<p class="rubric">What about cross-domain functions?</p>
<p>Some functions map from complex inputs to real outputs, or vice versa.
These functions form a special case of <a class="reference internal" href="#equation-4">(4)</a>, which we can derive using the
chain rule:</p>
<blockquote>
<div><ul>
<li><p>For <span class="math notranslate nohighlight">\(f: ℂ → ℝ\)</span>, we get:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z^*} = 2 * grad\_output * \frac{\partial s}{\partial z^{*}}\]</div>
</div></blockquote>
</li>
<li><p>For <span class="math notranslate nohighlight">\(f: ℝ → ℂ\)</span>, we get:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\frac{\partial L}{\partial z^*} = 2 * Re(grad\_out^* * \frac{\partial s}{\partial z^{*}})\]</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</section>
<section id="id9">
<h2>保存张量的挂钩<a class="headerlink" href="#id9" title="永久链接至标题">#</a></h2>
<p>您可以通过定义一对 <code class="docutils literal notranslate"><span class="pre">pack_hook</span> <span class="pre">/</span> <span class="pre">unpack_hook</span></code> 钩子来<a class="reference external" href="https://pytorch.org/docs/stable/notes/autograd.html#saved-tensors-doc">控制保存的张量如何打包/解包</a>。<code class="docutils literal notranslate"><span class="pre">pack_hook</span></code> 函数应该接受张量作为它的单个参数，但是可以返回任何 Python 对象（例如另一个张量，元组，甚至包含文件名的字符串）。<code class="docutils literal notranslate"><span class="pre">unpack_hook</span></code> 函数的唯一参数是 <code class="docutils literal notranslate"><span class="pre">pack_hook</span></code> 的输出，它应该返回一个张量，以便向后传递时使用。<code class="docutils literal notranslate"><span class="pre">unpack_hook</span></code> 返回的张量只需要与作为输入传递给 <code class="docutils literal notranslate"><span class="pre">pack_hook</span></code> 的张量具有相同的内容。特别是，任何与自动加载相关的元数据都可以被忽略，因为它们将在解包期间被覆盖。</p>
<p>这对组合的一个例子是：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">uuid</span>


<span class="k">class</span> <span class="nc">SelfDeletingTempFile</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tmp_dir</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tmp_dir</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()))</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">pack_hook</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">tmp_dir</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">):</span>
    <span class="n">temp_file</span> <span class="o">=</span> <span class="n">SelfDeletingTempFile</span><span class="p">(</span><span class="n">tmp_dir</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">temp_file</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">temp_file</span>


<span class="k">def</span> <span class="nf">unpack_hook</span><span class="p">(</span><span class="n">temp_file</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">temp_file</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>注意，<code class="docutils literal notranslate"><span class="pre">unpack_hook</span></code> 不应该删除临时文件，因为它可能会被多次调用：只要返回的 <code class="docutils literal notranslate"><span class="pre">SelfDeletingTempFile</span></code> 对象处于活动状态，该临时文件就应该处于活动状态。在上面的例子中，通过在不再需要临时文件时关闭它来防止泄漏（在删除 <code class="docutils literal notranslate"><span class="pre">SelfDeletingTempFile</span></code> 对象时）。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>我们保证 <code class="docutils literal notranslate"><span class="pre">pack_hook</span></code> 只被调用一次，但 <code class="docutils literal notranslate"><span class="pre">unpack_hook</span></code> 可以被调用多次，只要向后传递需要，并且我们希望它每次都返回相同的数据。</p>
</div>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>禁止对任何函数的输入执行就地操作，因为它们可能会导致意想不到的副作用。如果 pack 钩子的输入被就地修改，PyTorch 将抛出错误，但是没有捕捉到 unpack 钩子的输入被就地修改的情况。</p>
</div>
<section id="id10">
<h3>注册已保存张量的钩子<a class="headerlink" href="#id10" title="永久链接至标题">#</a></h3>
<p>通过调用已保存张量对象上的 <code class="xref py py-meth docutils literal notranslate"><span class="pre">register_hooks()</span></code> 方法，可以在已保存的张量上注册一对钩子。这些对象作为 <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> 的属性公开，并以 <code class="docutils literal notranslate"><span class="pre">_raw_saved_</span></code> 前缀开头。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_raw_saved_self</span><span class="o">.</span><span class="n">register_hooks</span><span class="p">(</span><span class="n">pack_hook</span><span class="p">,</span> <span class="n">unpack_hook</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>一旦注册了该对，就会调用 <code class="docutils literal notranslate"><span class="pre">pack_hook</span></code> 方法。每次需要访问保存的张量时，都会调用 <code class="docutils literal notranslate"><span class="pre">unpack_hook</span></code> 方法，或者通过 <code class="docutils literal notranslate"><span class="pre">y.grad_fn._saved_self</span></code> 或在向后传递时。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>如果你在保存的张量被释放之后（即在向后调用之后）保持对 <code class="docutils literal notranslate"><span class="pre">SavedTensor</span></code> 的引用，那么调用它的 <code class="xref py py-func docutils literal notranslate"><span class="pre">register_hooks()</span></code> 是被禁止的。PyTorch 在大多数情况下会抛出错误，但在某些情况下可能会失败，并可能出现未定义的行为。</p>
</div>
</section>
<section id="id11">
<h3>为保存的张量注册默认钩子<a class="headerlink" href="#id11" title="永久链接至标题">#</a></h3>
<p>或者，您可以使用上下文管理器 <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks()</span></code> 来注册一对钩子，这对钩子将应用于在该上下文中创建的所有保存的张量。</p>
<p>示例：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Only save on disk tensors that have size &gt;= 1000</span>
<span class="n">SAVE_ON_DISK_THRESHOLD</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">def</span> <span class="nf">pack_hook</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">SAVE_ON_DISK_THRESHOLD</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">tensor</span>
    <span class="n">temp_file</span> <span class="o">=</span> <span class="n">SelfDeletingTempFile</span><span class="p">(</span><span class="n">temp_file</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">temp_file</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">temp_file</span>

<span class="k">def</span> <span class="nf">unpack_hook</span><span class="p">(</span><span class="n">tensor_or_sctf</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_or_sctf</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tensor_or_sctf</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">tensor_or_sctf</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">saved_tensors_hooks</span><span class="p">(</span><span class="n">pack_hook</span><span class="p">,</span> <span class="n">unpack_hook</span><span class="p">):</span>
          <span class="c1"># ... compute output</span>
          <span class="n">output</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">return</span> <span class="n">output</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>用这个上下文管理器定义的钩子是线程局部的。因此，以下代码不会产生所需的效果，因为钩子没有经过 <code class="docutils literal notranslate"><span class="pre">DataParallel</span></code>。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example what NOT to do</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">saved_tensors_hooks</span><span class="p">(</span><span class="n">pack_hook</span><span class="p">,</span> <span class="n">unpack_hook</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>注意，使用这些钩子将禁用所有的优化以减少 Tensor 对象的创建。例如：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">saved_tensors_hooks</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
</pre></div>
</div>
<p>如果没有钩子，<code class="docutils literal notranslate"><span class="pre">x</span></code>, <code class="docutils literal notranslate"><span class="pre">y.grad_fn._saved_self</span></code> 和 <code class="docutils literal notranslate"><span class="pre">y.grad_fn._saved_other</span></code> 都指向同一个张量对象。通过钩子，PyTorch 将把 <code class="docutils literal notranslate"><span class="pre">x</span></code> 打包和解压到两个新的张量对象中，它们与原始的 <code class="docutils literal notranslate"><span class="pre">x</span></code> 共享相同的存储空间（没有执行拷贝）。</p>
</section>
</section>
</section>

<div class="section">
   
</div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="index.html" title="上一页 页">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">上一页</p>
            <p class="prev-next-title">笔记</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="detection.html" title="下一页 页">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">下一页</p>
        <p class="prev-next-title">目标检测</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By xinetzone<br/>
  
      &copy; Copyright 2021, xinetzone.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>